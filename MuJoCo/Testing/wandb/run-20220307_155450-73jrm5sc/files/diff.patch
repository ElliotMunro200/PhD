diff --git a/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/DQN_Classes.py
index db77622..929966f 100644
--- a/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -252,8 +252,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         # env = SDP_env()
-        # num_states = env.num_states
-        # num_actions = env.num_actions
+        # dim_states = env.dim_states
+        # dim_actions = env.dim_actions
         env = gym.make(config["env_name"])
         num_states = env.observation_space.shape[0]
         num_actions = env.action_space.n
@@ -265,8 +265,8 @@ if __name__ == "__main__":
     #h-DQN
     if h_DQN:
         # env = SDP_env()
-        # num_states = env.num_states
-        # num_actions = env.num_actions
+        # dim_states = env.dim_states
+        # dim_actions = env.dim_actions
         goal_state_rep_f = 2
         env = gym.make(config["env_name"])
         num_goals = env.observation_space.shape[0]
diff --git a/Modern Implementations/DQN_etc/Q-vs-DQN-vs-hDQN.py b/Modern Implementations/DQN_etc/Q-vs-DQN-vs-hDQN.py
index 2a1f564..cc8025b 100644
--- a/Modern Implementations/DQN_etc/Q-vs-DQN-vs-hDQN.py	
+++ b/Modern Implementations/DQN_etc/Q-vs-DQN-vs-hDQN.py	
@@ -19,7 +19,7 @@ def Q_learning_run(env, num_episodes, steps_per_episode):
     alpha = 0.25
     discount = 0.99
     #initialize Q-values, rewards, epsilons for training
-    Qs = np.zeros((env.num_states, env.num_actions))
+    Qs = np.zeros((env.dim_states, env.dim_actions))
     rewards_by_episode = []
     epsilons = []
     for episode in range(num_episodes):
@@ -34,7 +34,7 @@ def Q_learning_run(env, num_episodes, steps_per_episode):
             rand_num = random.random()
             #explore
             if rand_num < epsilon:
-                action = random.sample(range(env.num_actions),1)
+                action = random.sample(range(env.dim_actions), 1)
             #exploit
             else:
                 action = np.argmax(Qs[state-1,:])
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211015_223243-34bc75dr/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211015_223243-34bc75dr/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 2a9a2c8..3220165 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211015_223243-34bc75dr/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211015_223243-34bc75dr/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -231,8 +231,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = gym.make(config["env_name"]) #SDP_env()
-        num_states = env.num_states
-        num_actions = env.num_actions
+        num_states = env.dim_states
+        num_actions = env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -242,8 +242,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env()
-        num_goals = env.num_states
-        num_actions = env.num_actions
+        num_goals = env.dim_states
+        num_actions = env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211018_170620-iynry50s/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211018_170620-iynry50s/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 2a9a2c8..3220165 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211018_170620-iynry50s/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211018_170620-iynry50s/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -231,8 +231,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = gym.make(config["env_name"]) #SDP_env()
-        num_states = env.num_states
-        num_actions = env.num_actions
+        num_states = env.dim_states
+        num_actions = env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -242,8 +242,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env()
-        num_goals = env.num_states
-        num_actions = env.num_actions
+        num_goals = env.dim_states
+        num_actions = env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211018_171712-q77sy9ht/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211018_171712-q77sy9ht/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 91fa580..d1a42ac 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211018_171712-q77sy9ht/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211018_171712-q77sy9ht/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -231,8 +231,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = gym.make(config["env_name"]) #SDP_env()
-        num_states = env.observation_space.shape[0] #env.num_states
-        num_actions = env.action_space.n #env.num_actions
+        num_states = env.observation_space.shape[0] #env.dim_states
+        num_actions = env.action_space.n #env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -242,8 +242,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env()
-        num_goals = env.num_states
-        num_actions = env.num_actions
+        num_goals = env.dim_states
+        num_actions = env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211021_094832-1704882d/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211021_094832-1704882d/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index ad10ee5..f579e4e 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211021_094832-1704882d/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211021_094832-1704882d/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -232,8 +232,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = gym.make(config["env_name"])  #SDP_env()
-        num_states = env.observation_space.shape[0]  #env.num_states
-        num_actions = env.action_space.n  #env.num_actions
+        num_states = env.observation_space.shape[0]  #env.dim_states
+        num_actions = env.action_space.n  #env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -243,8 +243,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env()
-        num_goals = env.num_states
-        num_actions = env.num_actions
+        num_goals = env.dim_states
+        num_actions = env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211021_095621-119jkrae/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211021_095621-119jkrae/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 10b4047..496c27e 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211021_095621-119jkrae/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211021_095621-119jkrae/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -237,8 +237,8 @@ if __name__ == "__main__":
         env = Monitor(env)
         env = DummyVecEnv([make_env])
         env = VecVideoRecorder(env, f"videos/{run.id}", record_video_trigger=lambda x: x % 2000 == 0, video_length=200)
-        num_states = env.observation_space.shape[0]  #env.num_states
-        num_actions = env.action_space.n  #env.num_actions
+        num_states = env.observation_space.shape[0]  #env.dim_states
+        num_actions = env.action_space.n  #env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -248,8 +248,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env()
-        num_goals = env.num_states
-        num_actions = env.num_actions
+        num_goals = env.dim_states
+        num_actions = env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211021_095711-4dqb124u/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211021_095711-4dqb124u/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index fc08d63..038fd38 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211021_095711-4dqb124u/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211021_095711-4dqb124u/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -237,8 +237,8 @@ if __name__ == "__main__":
         env = Monitor(env)
         env = DummyVecEnv([env])
         env = VecVideoRecorder(env, f"videos/{run.id}", record_video_trigger=lambda x: x % 2000 == 0, video_length=200)
-        num_states = env.observation_space.shape[0]  #env.num_states
-        num_actions = env.action_space.n  #env.num_actions
+        num_states = env.observation_space.shape[0]  #env.dim_states
+        num_actions = env.action_space.n  #env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -248,8 +248,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env()
-        num_goals = env.num_states
-        num_actions = env.num_actions
+        num_goals = env.dim_states
+        num_actions = env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211021_095954-6tvcd3v0/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211021_095954-6tvcd3v0/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 0a8483c..48e9b2f 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211021_095954-6tvcd3v0/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211021_095954-6tvcd3v0/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -240,8 +240,8 @@ if __name__ == "__main__":
     if DQN:
         env = DummyVecEnv([make_env])
         env = VecVideoRecorder(env, f"videos/{run.id}", record_video_trigger=lambda x: x % 2000 == 0, video_length=200)
-        num_states = env.observation_space.shape[0]  #env.num_states
-        num_actions = env.action_space.n  #env.num_actions
+        num_states = env.observation_space.shape[0]  #env.dim_states
+        num_actions = env.action_space.n  #env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -251,8 +251,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env()
-        num_goals = env.num_states
-        num_actions = env.num_actions
+        num_goals = env.dim_states
+        num_actions = env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211021_100542-2wtzro2q/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211021_100542-2wtzro2q/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 0999130..eda7193 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211021_100542-2wtzro2q/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211021_100542-2wtzro2q/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -233,8 +233,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = gym.make(config["env_name"])
-        num_states = env.observation_space.shape[0]  #env.num_states
-        num_actions = env.action_space.n  #env.num_actions
+        num_states = env.observation_space.shape[0]  #env.dim_states
+        num_actions = env.action_space.n  #env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -244,8 +244,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env()
-        num_goals = env.num_states
-        num_actions = env.num_actions
+        num_goals = env.dim_states
+        num_actions = env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211021_102128-1mo731sb/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211021_102128-1mo731sb/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 8620a6c..0cc340a 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211021_102128-1mo731sb/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211021_102128-1mo731sb/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -241,8 +241,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = gym.make(config["env_name"])
-        num_states = env.observation_space.shape[0]  #env.num_states
-        num_actions = env.action_space.n  #env.num_actions
+        num_states = env.observation_space.shape[0]  #env.dim_states
+        num_actions = env.action_space.n  #env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -252,8 +252,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env()
-        num_goals = env.num_states
-        num_actions = env.num_actions
+        num_goals = env.dim_states
+        num_actions = env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211021_104229-2h3u35ah/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211021_104229-2h3u35ah/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index ef677e4..0b38e79 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211021_104229-2h3u35ah/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211021_104229-2h3u35ah/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -243,8 +243,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = gym.make(config["env_name"])
-        num_states = env.observation_space.shape[0]  #env.num_states
-        num_actions = env.action_space.n  #env.num_actions
+        num_states = env.observation_space.shape[0]  #env.dim_states
+        num_actions = env.action_space.n  #env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -254,8 +254,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env()
-        num_goals = env.num_states
-        num_actions = env.num_actions
+        num_goals = env.dim_states
+        num_actions = env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211021_104325-v3eihvm4/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211021_104325-v3eihvm4/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index c9fbf80..569aca9 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211021_104325-v3eihvm4/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211021_104325-v3eihvm4/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -243,8 +243,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = gym.make(config["env_name"])
-        num_states = env.observation_space.shape[0]  #env.num_states
-        num_actions = env.action_space.n  #env.num_actions
+        num_states = env.observation_space.shape[0]  #env.dim_states
+        num_actions = env.action_space.n  #env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -254,8 +254,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env()
-        num_goals = env.num_states
-        num_actions = env.num_actions
+        num_goals = env.dim_states
+        num_actions = env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211021_104636-2a0k02r1/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211021_104636-2a0k02r1/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index c9fbf80..569aca9 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211021_104636-2a0k02r1/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211021_104636-2a0k02r1/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -243,8 +243,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = gym.make(config["env_name"])
-        num_states = env.observation_space.shape[0]  #env.num_states
-        num_actions = env.action_space.n  #env.num_actions
+        num_states = env.observation_space.shape[0]  #env.dim_states
+        num_actions = env.action_space.n  #env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -254,8 +254,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env()
-        num_goals = env.num_states
-        num_actions = env.num_actions
+        num_goals = env.dim_states
+        num_actions = env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211021_104713-38m4wiuy/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211021_104713-38m4wiuy/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index c9fbf80..569aca9 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211021_104713-38m4wiuy/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211021_104713-38m4wiuy/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -243,8 +243,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = gym.make(config["env_name"])
-        num_states = env.observation_space.shape[0]  #env.num_states
-        num_actions = env.action_space.n  #env.num_actions
+        num_states = env.observation_space.shape[0]  #env.dim_states
+        num_actions = env.action_space.n  #env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -254,8 +254,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env()
-        num_goals = env.num_states
-        num_actions = env.num_actions
+        num_goals = env.dim_states
+        num_actions = env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211021_104748-2a9huu3l/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211021_104748-2a9huu3l/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index c9fbf80..569aca9 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211021_104748-2a9huu3l/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211021_104748-2a9huu3l/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -243,8 +243,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = gym.make(config["env_name"])
-        num_states = env.observation_space.shape[0]  #env.num_states
-        num_actions = env.action_space.n  #env.num_actions
+        num_states = env.observation_space.shape[0]  #env.dim_states
+        num_actions = env.action_space.n  #env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -254,8 +254,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env()
-        num_goals = env.num_states
-        num_actions = env.num_actions
+        num_goals = env.dim_states
+        num_actions = env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211021_104827-22qfu2d0/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211021_104827-22qfu2d0/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index c9fbf80..569aca9 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211021_104827-22qfu2d0/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211021_104827-22qfu2d0/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -243,8 +243,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = gym.make(config["env_name"])
-        num_states = env.observation_space.shape[0]  #env.num_states
-        num_actions = env.action_space.n  #env.num_actions
+        num_states = env.observation_space.shape[0]  #env.dim_states
+        num_actions = env.action_space.n  #env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -254,8 +254,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env()
-        num_goals = env.num_states
-        num_actions = env.num_actions
+        num_goals = env.dim_states
+        num_actions = env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211021_104859-spc2vg8o/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211021_104859-spc2vg8o/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index c9fbf80..569aca9 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211021_104859-spc2vg8o/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211021_104859-spc2vg8o/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -243,8 +243,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = gym.make(config["env_name"])
-        num_states = env.observation_space.shape[0]  #env.num_states
-        num_actions = env.action_space.n  #env.num_actions
+        num_states = env.observation_space.shape[0]  #env.dim_states
+        num_actions = env.action_space.n  #env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -254,8 +254,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env()
-        num_goals = env.num_states
-        num_actions = env.num_actions
+        num_goals = env.dim_states
+        num_actions = env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211021_121449-35dwadsw/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211021_121449-35dwadsw/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index f7ce983..9524d66 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211021_121449-35dwadsw/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211021_121449-35dwadsw/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -250,8 +250,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = gym.make(config["env_name"])
-        num_states = env.observation_space.shape[0]  #env.num_states
-        num_actions = env.action_space.n  #env.num_actions
+        num_states = env.observation_space.shape[0]  #env.dim_states
+        num_actions = env.action_space.n  #env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -261,8 +261,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = env = gym.make(config["env_name"]) #SDP_env()
-        num_states = env.observation_space.shape[0]  # env.num_states
-        num_actions = env.action_space.n  # env.num_actions
+        num_states = env.observation_space.shape[0]  # env.dim_states
+        num_actions = env.action_space.n  # env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211021_121710-333gcscu/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211021_121710-333gcscu/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index a810977..e4bcc0d 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211021_121710-333gcscu/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211021_121710-333gcscu/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -250,8 +250,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = gym.make(config["env_name"])
-        num_states = env.observation_space.shape[0]  #env.num_states
-        num_actions = env.action_space.n  #env.num_actions
+        num_states = env.observation_space.shape[0]  #env.dim_states
+        num_actions = env.action_space.n  #env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -261,8 +261,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = gym.make(config["env_name"]) #SDP_env()
-        num_goals = env.observation_space.shape[0]  # env.num_states
-        num_actions = env.action_space.n  # env.num_actions
+        num_goals = env.observation_space.shape[0]  # env.dim_states
+        num_actions = env.action_space.n  # env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211021_122007-1v21ub1v/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211021_122007-1v21ub1v/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index aa76285..fe7d53c 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211021_122007-1v21ub1v/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211021_122007-1v21ub1v/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -250,8 +250,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = gym.make(config["env_name"])
-        num_states = env.observation_space.shape[0]  #env.num_states
-        num_actions = env.action_space.n  #env.num_actions
+        num_states = env.observation_space.shape[0]  #env.dim_states
+        num_actions = env.action_space.n  #env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -261,8 +261,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = gym.make(config["env_name"]) #SDP_env()
-        num_goals = env.observation_space.shape[0]  # env.num_states
-        num_actions = env.action_space.n  # env.num_actions
+        num_goals = env.observation_space.shape[0]  # env.dim_states
+        num_actions = env.action_space.n  # env.dim_actions
         print(num_goals, num_actions)
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211021_122200-3l59msa2/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211021_122200-3l59msa2/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index a810977..e4bcc0d 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211021_122200-3l59msa2/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211021_122200-3l59msa2/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -250,8 +250,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = gym.make(config["env_name"])
-        num_states = env.observation_space.shape[0]  #env.num_states
-        num_actions = env.action_space.n  #env.num_actions
+        num_states = env.observation_space.shape[0]  #env.dim_states
+        num_actions = env.action_space.n  #env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -261,8 +261,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = gym.make(config["env_name"]) #SDP_env()
-        num_goals = env.observation_space.shape[0]  # env.num_states
-        num_actions = env.action_space.n  # env.num_actions
+        num_goals = env.observation_space.shape[0]  # env.dim_states
+        num_actions = env.action_space.n  # env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211021_122743-aq72d6gp/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211021_122743-aq72d6gp/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 3b2a065..cde2545 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211021_122743-aq72d6gp/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211021_122743-aq72d6gp/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -250,8 +250,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = gym.make(config["env_name"])
-        num_states = env.observation_space.shape[0]  #env.num_states
-        num_actions = env.action_space.n  #env.num_actions
+        num_states = env.observation_space.shape[0]  #env.dim_states
+        num_actions = env.action_space.n  #env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -261,8 +261,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = gym.make(config["env_name"]) #SDP_env()
-        num_goals = env.observation_space.shape[0]  # env.num_states
-        num_actions = env.action_space.n  # env.num_actions
+        num_goals = env.observation_space.shape[0]  # env.dim_states
+        num_actions = env.action_space.n  # env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211021_122822-2kd4eryc/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211021_122822-2kd4eryc/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 53f85ae..954566e 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211021_122822-2kd4eryc/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211021_122822-2kd4eryc/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -250,8 +250,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = gym.make(config["env_name"])
-        num_states = env.observation_space.shape[0]  #env.num_states
-        num_actions = env.action_space.n  #env.num_actions
+        num_states = env.observation_space.shape[0]  #env.dim_states
+        num_actions = env.action_space.n  #env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -261,8 +261,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = gym.make(config["env_name"]) #SDP_env()
-        num_goals = env.observation_space.shape[0]  # env.num_states
-        num_actions = env.action_space.n  # env.num_actions
+        num_goals = env.observation_space.shape[0]  # env.dim_states
+        num_actions = env.action_space.n  # env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_103054-1br9qhlv/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_103054-1br9qhlv/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 05563b5..a04b425 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_103054-1br9qhlv/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_103054-1br9qhlv/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -251,8 +251,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = gym.make(config["env_name"])
-        num_states = env.observation_space.shape[0]  #env.num_states
-        num_actions = env.action_space.n  #env.num_actions
+        num_states = env.observation_space.shape[0]  #env.dim_states
+        num_actions = env.action_space.n  #env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -262,8 +262,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = gym.make(config["env_name"]) #SDP_env()
-        num_goals = env.observation_space.shape[0]  # env.num_states
-        num_actions = env.action_space.n  # env.num_actions
+        num_goals = env.observation_space.shape[0]  # env.dim_states
+        num_actions = env.action_space.n  # env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_110421-3kve16g1/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_110421-3kve16g1/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 05563b5..a04b425 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_110421-3kve16g1/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_110421-3kve16g1/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -251,8 +251,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = gym.make(config["env_name"])
-        num_states = env.observation_space.shape[0]  #env.num_states
-        num_actions = env.action_space.n  #env.num_actions
+        num_states = env.observation_space.shape[0]  #env.dim_states
+        num_actions = env.action_space.n  #env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -262,8 +262,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = gym.make(config["env_name"]) #SDP_env()
-        num_goals = env.observation_space.shape[0]  # env.num_states
-        num_actions = env.action_space.n  # env.num_actions
+        num_goals = env.observation_space.shape[0]  # env.dim_states
+        num_actions = env.action_space.n  # env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_110455-39nnhjai/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_110455-39nnhjai/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index b1f3aa2..f2ccb20 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_110455-39nnhjai/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_110455-39nnhjai/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -251,8 +251,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = gym.make(config["env_name"])
-        num_states = env.observation_space.shape[0]  #env.num_states
-        num_actions = env.action_space.n  #env.num_actions
+        num_states = env.observation_space.shape[0]  #env.dim_states
+        num_actions = env.action_space.n  #env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -262,8 +262,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = gym.make(config["env_name"]) #SDP_env()
-        num_goals = env.observation_space.shape[0]  # env.num_states
-        num_actions = env.action_space.n  # env.num_actions
+        num_goals = env.observation_space.shape[0]  # env.dim_states
+        num_actions = env.action_space.n  # env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_110624-fpbqd0mg/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_110624-fpbqd0mg/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index f04bae7..31d54d1 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_110624-fpbqd0mg/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_110624-fpbqd0mg/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -251,8 +251,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = gym.make(config["env_name"])
-        num_states = env.observation_space.shape[0]  #env.num_states
-        num_actions = env.action_space.n  #env.num_actions
+        num_states = env.observation_space.shape[0]  #env.dim_states
+        num_actions = env.action_space.n  #env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -262,8 +262,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = gym.make(config["env_name"]) #SDP_env()
-        num_goals = env.observation_space.shape[0]  # env.num_states
-        num_actions = env.action_space.n  # env.num_actions
+        num_goals = env.observation_space.shape[0]  # env.dim_states
+        num_actions = env.action_space.n  # env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_110740-2cjwtbmo/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_110740-2cjwtbmo/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index d3f99a1..e730bfc 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_110740-2cjwtbmo/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_110740-2cjwtbmo/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -251,8 +251,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = gym.make(config["env_name"])
-        num_states = env.observation_space.shape[0]  #env.num_states
-        num_actions = env.action_space.n  #env.num_actions
+        num_states = env.observation_space.shape[0]  #env.dim_states
+        num_actions = env.action_space.n  #env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -262,8 +262,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = gym.make(config["env_name"]) #SDP_env()
-        num_goals = env.observation_space.shape[0]  # env.num_states
-        num_actions = env.action_space.n  # env.num_actions
+        num_goals = env.observation_space.shape[0]  # env.dim_states
+        num_actions = env.action_space.n  # env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_111739-39xaclcx/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_111739-39xaclcx/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 2cb490e..a5a725e 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_111739-39xaclcx/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_111739-39xaclcx/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -251,8 +251,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = gym.make(config["env_name"])
-        num_states = env.observation_space.shape[0]  #env.num_states
-        num_actions = env.action_space.n  #env.num_actions
+        num_states = env.observation_space.shape[0]  #env.dim_states
+        num_actions = env.action_space.n  #env.dim_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -262,8 +262,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = gym.make(config["env_name"]) #SDP_env()
-        num_goals = env.observation_space.shape[0]  # env.num_states
-        num_actions = env.action_space.n  # env.num_actions
+        num_goals = env.observation_space.shape[0]  # env.dim_states
+        num_actions = env.action_space.n  # env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_113847-3o0ghh9q/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_113847-3o0ghh9q/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 1ed85d2..90a001e 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_113847-3o0ghh9q/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_113847-3o0ghh9q/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -251,8 +251,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = SDP_env() #gym.make(config["env_name"])
-        num_states = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_states = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -262,8 +262,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = gym.make(config["env_name"]) #SDP_env()
-        num_goals = env.observation_space.shape[0]  #env.num_states
-        num_actions = env.action_space.n  #env.num_actions
+        num_goals = env.observation_space.shape[0]  #env.dim_states
+        num_actions = env.action_space.n  #env.dim_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_114251-3727fk33/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_114251-3727fk33/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 55d0e1f..f8bd72f 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_114251-3727fk33/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_114251-3727fk33/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -251,8 +251,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = SDP_env() #gym.make(config["env_name"])
-        num_states = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_states = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -262,8 +262,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env() #gym.make(config["env_name"])
-        num_goals = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_goals = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_115804-1wbz2mfr/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_115804-1wbz2mfr/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 75fdcda..5bf0022 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_115804-1wbz2mfr/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_115804-1wbz2mfr/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -252,8 +252,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = SDP_env() #gym.make(config["env_name"])
-        num_states = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_states = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -263,8 +263,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env() #gym.make(config["env_name"])
-        num_goals = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_goals = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_115942-218ld2iv/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_115942-218ld2iv/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 7aabf21..4670d48 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_115942-218ld2iv/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_115942-218ld2iv/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -252,8 +252,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = SDP_env() #gym.make(config["env_name"])
-        num_states = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_states = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -263,8 +263,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env() #gym.make(config["env_name"])
-        num_goals = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_goals = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_120008-d7rh51kt/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_120008-d7rh51kt/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 75fdcda..5bf0022 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_120008-d7rh51kt/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_120008-d7rh51kt/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -252,8 +252,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = SDP_env() #gym.make(config["env_name"])
-        num_states = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_states = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -263,8 +263,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env() #gym.make(config["env_name"])
-        num_goals = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_goals = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_120221-18sygrpe/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_120221-18sygrpe/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 4bd67c2..70540ec 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_120221-18sygrpe/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_120221-18sygrpe/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -252,8 +252,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = SDP_env() #gym.make(config["env_name"])
-        num_states = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_states = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -263,8 +263,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env() #gym.make(config["env_name"])
-        num_goals = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_goals = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_120308-1xmaudqg/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_120308-1xmaudqg/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 2403e66..95af98d 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_120308-1xmaudqg/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_120308-1xmaudqg/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -252,8 +252,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = SDP_env() #gym.make(config["env_name"])
-        num_states = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_states = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -263,8 +263,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env() #gym.make(config["env_name"])
-        num_goals = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_goals = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_120535-37w8vahg/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_120535-37w8vahg/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 2403e66..95af98d 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_120535-37w8vahg/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_120535-37w8vahg/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -252,8 +252,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = SDP_env() #gym.make(config["env_name"])
-        num_states = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_states = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -263,8 +263,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env() #gym.make(config["env_name"])
-        num_goals = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_goals = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_120713-1n8mb1gl/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_120713-1n8mb1gl/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 2dd39c7..ff85b34 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_120713-1n8mb1gl/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_120713-1n8mb1gl/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -251,8 +251,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = SDP_env() #gym.make(config["env_name"])
-        num_states = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_states = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -262,8 +262,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env() #gym.make(config["env_name"])
-        num_goals = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_goals = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_120902-mrcrqd84/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_120902-mrcrqd84/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 0099882..a2d889e 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_120902-mrcrqd84/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_120902-mrcrqd84/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -252,8 +252,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = SDP_env() #gym.make(config["env_name"])
-        num_states = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_states = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -263,8 +263,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env() #gym.make(config["env_name"])
-        num_goals = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_goals = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_121027-2zlwbxb9/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_121027-2zlwbxb9/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 1163c1e..dad7d46 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_121027-2zlwbxb9/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_121027-2zlwbxb9/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -253,8 +253,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = SDP_env() #gym.make(config["env_name"])
-        num_states = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_states = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -264,8 +264,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env() #gym.make(config["env_name"])
-        num_goals = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_goals = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_121125-kxlynyy7/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_121125-kxlynyy7/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 4cc5f63..a3bc36f 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_121125-kxlynyy7/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_121125-kxlynyy7/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -251,8 +251,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = SDP_env() #gym.make(config["env_name"])
-        num_states = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_states = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -262,8 +262,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env() #gym.make(config["env_name"])
-        num_goals = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_goals = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_121347-hv36kltz/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_121347-hv36kltz/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 5a5c239..6255bc0 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_121347-hv36kltz/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_121347-hv36kltz/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -251,8 +251,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = SDP_env() #gym.make(config["env_name"])
-        num_states = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_states = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -262,8 +262,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env() #gym.make(config["env_name"])
-        num_goals = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_goals = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_121443-307f1of8/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_121443-307f1of8/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 34ad5b7..5ec056d 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_121443-307f1of8/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_121443-307f1of8/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -251,8 +251,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = SDP_env() #gym.make(config["env_name"])
-        num_states = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_states = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -262,8 +262,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env() #gym.make(config["env_name"])
-        num_goals = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_goals = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_121530-1eitf265/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_121530-1eitf265/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 0c929b5..6727167 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_121530-1eitf265/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_121530-1eitf265/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -251,8 +251,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         env = SDP_env() #gym.make(config["env_name"])
-        num_states = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_states = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -262,8 +262,8 @@ if __name__ == "__main__":
     if h_DQN:
         goal_state_rep_f = 2
         env = SDP_env() #gym.make(config["env_name"])
-        num_goals = env.num_states #env.observation_space.shape[0]
-        num_actions = env.num_actions #env.action_space.n
+        num_goals = env.dim_states #env.observation_space.shape[0]
+        num_actions = env.dim_actions #env.action_space.n
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_141754-3bif548k/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_141754-3bif548k/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 7bb3cd7..1a6ca12 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_141754-3bif548k/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_141754-3bif548k/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -251,8 +251,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         # env = SDP_env()
-        # num_states = env.num_states
-        # num_actions = env.num_actions
+        # dim_states = env.dim_states
+        # dim_actions = env.dim_actions
         env = gym.make(config["env_name"])
         num_states = env.observation_space.shape[0]
         num_actions = env.action_space.n
@@ -264,8 +264,8 @@ if __name__ == "__main__":
     #h-DQN
     if h_DQN:
         # env = SDP_env()
-        # num_states = env.num_states
-        # num_actions = env.num_actions
+        # dim_states = env.dim_states
+        # dim_actions = env.dim_actions
         goal_state_rep_f = 2
         env = gym.make(config["env_name"])
         num_goals = env.observation_space.shape[0]
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_144242-2v8hi4mt/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_144242-2v8hi4mt/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 7bb3cd7..1a6ca12 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_144242-2v8hi4mt/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_144242-2v8hi4mt/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -251,8 +251,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         # env = SDP_env()
-        # num_states = env.num_states
-        # num_actions = env.num_actions
+        # dim_states = env.dim_states
+        # dim_actions = env.dim_actions
         env = gym.make(config["env_name"])
         num_states = env.observation_space.shape[0]
         num_actions = env.action_space.n
@@ -264,8 +264,8 @@ if __name__ == "__main__":
     #h-DQN
     if h_DQN:
         # env = SDP_env()
-        # num_states = env.num_states
-        # num_actions = env.num_actions
+        # dim_states = env.dim_states
+        # dim_actions = env.dim_actions
         goal_state_rep_f = 2
         env = gym.make(config["env_name"])
         num_goals = env.observation_space.shape[0]
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_145431-1cp0ckat/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_145431-1cp0ckat/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 51e6215..796e4bf 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_145431-1cp0ckat/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_145431-1cp0ckat/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -251,8 +251,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         # env = SDP_env()
-        # num_states = env.num_states
-        # num_actions = env.num_actions
+        # dim_states = env.dim_states
+        # dim_actions = env.dim_actions
         env = gym.make(config["env_name"])
         num_states = env.observation_space.shape[0]
         num_actions = env.action_space.n
@@ -264,8 +264,8 @@ if __name__ == "__main__":
     #h-DQN
     if h_DQN:
         # env = SDP_env()
-        # num_states = env.num_states
-        # num_actions = env.num_actions
+        # dim_states = env.dim_states
+        # dim_actions = env.dim_actions
         goal_state_rep_f = 2
         env = gym.make(config["env_name"])
         num_goals = env.observation_space.shape[0]
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_145719-3e019xkj/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_145719-3e019xkj/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index c63aa63..14c51bd 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_145719-3e019xkj/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_145719-3e019xkj/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -251,8 +251,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         # env = SDP_env()
-        # num_states = env.num_states
-        # num_actions = env.num_actions
+        # dim_states = env.dim_states
+        # dim_actions = env.dim_actions
         env = gym.make(config["env_name"])
         num_states = env.observation_space.shape[0]
         num_actions = env.action_space.n
@@ -264,8 +264,8 @@ if __name__ == "__main__":
     #h-DQN
     if h_DQN:
         # env = SDP_env()
-        # num_states = env.num_states
-        # num_actions = env.num_actions
+        # dim_states = env.dim_states
+        # dim_actions = env.dim_actions
         goal_state_rep_f = 2
         env = gym.make(config["env_name"])
         num_goals = env.observation_space.shape[0]
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_145749-3gg2spey/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_145749-3gg2spey/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 7bb3cd7..1a6ca12 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_145749-3gg2spey/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_145749-3gg2spey/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -251,8 +251,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         # env = SDP_env()
-        # num_states = env.num_states
-        # num_actions = env.num_actions
+        # dim_states = env.dim_states
+        # dim_actions = env.dim_actions
         env = gym.make(config["env_name"])
         num_states = env.observation_space.shape[0]
         num_actions = env.action_space.n
@@ -264,8 +264,8 @@ if __name__ == "__main__":
     #h-DQN
     if h_DQN:
         # env = SDP_env()
-        # num_states = env.num_states
-        # num_actions = env.num_actions
+        # dim_states = env.dim_states
+        # dim_actions = env.dim_actions
         goal_state_rep_f = 2
         env = gym.make(config["env_name"])
         num_goals = env.observation_space.shape[0]
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_150228-207ziiuv/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_150228-207ziiuv/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 51e6215..796e4bf 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_150228-207ziiuv/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_150228-207ziiuv/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -251,8 +251,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         # env = SDP_env()
-        # num_states = env.num_states
-        # num_actions = env.num_actions
+        # dim_states = env.dim_states
+        # dim_actions = env.dim_actions
         env = gym.make(config["env_name"])
         num_states = env.observation_space.shape[0]
         num_actions = env.action_space.n
@@ -264,8 +264,8 @@ if __name__ == "__main__":
     #h-DQN
     if h_DQN:
         # env = SDP_env()
-        # num_states = env.num_states
-        # num_actions = env.num_actions
+        # dim_states = env.dim_states
+        # dim_actions = env.dim_actions
         goal_state_rep_f = 2
         env = gym.make(config["env_name"])
         num_goals = env.observation_space.shape[0]
diff --git a/Modern Implementations/DQN_etc/wandb/run-20211027_150347-1b6kiupo/files/code/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/wandb/run-20211027_150347-1b6kiupo/files/code/Modern Implementations/DQN_etc/DQN_Classes.py
index 51e6215..796e4bf 100644
--- a/Modern Implementations/DQN_etc/wandb/run-20211027_150347-1b6kiupo/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/wandb/run-20211027_150347-1b6kiupo/files/code/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -251,8 +251,8 @@ if __name__ == "__main__":
     #DQN
     if DQN:
         # env = SDP_env()
-        # num_states = env.num_states
-        # num_actions = env.num_actions
+        # dim_states = env.dim_states
+        # dim_actions = env.dim_actions
         env = gym.make(config["env_name"])
         num_states = env.observation_space.shape[0]
         num_actions = env.action_space.n
@@ -264,8 +264,8 @@ if __name__ == "__main__":
     #h-DQN
     if h_DQN:
         # env = SDP_env()
-        # num_states = env.num_states
-        # num_actions = env.num_actions
+        # dim_states = env.dim_states
+        # dim_actions = env.dim_actions
         goal_state_rep_f = 2
         env = gym.make(config["env_name"])
         num_goals = env.observation_space.shape[0]
diff --git a/Modern Implementations/MuJoCo/Testing/C_L_RoboLoco_Agent.py b/Modern Implementations/MuJoCo/Testing/C_L_RoboLoco_Agent.py
new file mode 100644
index 0000000..1e53d03
--- /dev/null
+++ b/Modern Implementations/MuJoCo/Testing/C_L_RoboLoco_Agent.py	
@@ -0,0 +1,93 @@
+#Defining the Robotic Locomotion agent
+import PIL.Image
+import numpy as np
+import matplotlib.pyplot as plt
+
+from dm_control import mjcf
+
+from dm_control import composer
+from dm_control.composer import variation
+from dm_control.composer.observation import observable
+
+from dm_control.locomotion.arenas import floors
+from dm_control.composer.variation import noises
+from dm_control.composer.variation import distributions
+
+class Leg(object):
+  """A 2-DoF leg with position actuators."""
+  def __init__(self, length, rgba):
+    self.model = mjcf.RootElement()
+
+    # Defaults:
+    self.model.default.joint.damping = 2
+    self.model.default.joint.type = 'hinge'
+    self.model.default.geom.type = 'capsule'
+    self.model.default.geom.rgba = rgba  # Continued below...
+
+    # Thigh:
+    self.thigh = self.model.worldbody.add('body')
+    self.hip = self.thigh.add('joint', axis=[0, 0, 1])
+    self.thigh.add('geom', fromto=[0, 0, 0, length, 0, 0], size=[length/4])
+
+    # Hip:
+    self.shin = self.thigh.add('body', pos=[length, 0, 0])
+    self.knee = self.shin.add('joint', axis=[0, 1, 0])
+    self.shin.add('geom', fromto=[0, 0, 0, 0, 0, -length], size=[length/5])
+
+    # Position actuators:
+    self.model.actuator.add('position', joint=self.hip, kp=10)
+    self.model.actuator.add('position', joint=self.knee, kp=10)
+
+def make_roboloco(num_legs, BODY_SIZE, BODY_RADIUS):
+  """Constructs a locomotion robot with `num_legs` legs."""
+  random_state = np.random.RandomState(42)
+  rgba = random_state.uniform([0, 0, 0, 1], [1, 1, 1, 1])
+  model = mjcf.RootElement()
+  model.compiler.angle = 'radian'  # Use radians.
+
+  # Make the torso geom.
+  model.worldbody.add(
+      'geom', name='torso', type='ellipsoid', size=BODY_SIZE, rgba=rgba)
+
+  # Attach legs to equidistant sites on the circumference.
+  for i in range(num_legs):
+    theta = 2 * i * np.pi / num_legs
+    hip_pos = BODY_RADIUS * np.array([np.cos(theta), np.sin(theta), 0])
+    hip_site = model.worldbody.add('site', pos=hip_pos, euler=[0, 0, theta])
+    leg = Leg(length=BODY_RADIUS, rgba=rgba)
+    hip_site.attach(leg.model)
+
+  return model
+
+#@title The `RoboLoco` class
+
+
+class RoboLoco(composer.Entity):
+  """A multi-legged robot for locomotion derived from `composer.Entity`."""
+  def _build(self, num_legs, body_size, body_radius):
+    self._model = make_roboloco(num_legs, body_size, body_radius)
+
+  def _build_observables(self):
+    return RoboLocoObservables(self)
+
+  @property
+  def mjcf_model(self):
+    return self._model
+
+  @property
+  def actuators(self):
+    return tuple(self._model.find_all('actuator'))
+
+
+# Add simple observable features for joint angles and velocities.
+class RoboLocoObservables(composer.Observables):
+
+  @composer.observable
+  def joint_positions(self):
+    all_joints = self._entity.mjcf_model.find_all('joint')
+    return observable.MJCFFeature('qpos', all_joints)
+
+  @composer.observable
+  def joint_velocities(self):
+    all_joints = self._entity.mjcf_model.find_all('joint')
+    return observable.MJCFFeature('qvel', all_joints)
\ No newline at end of file
diff --git a/Modern Implementations/MuJoCo/Testing/C_L_RoboLoco_Learning.py b/Modern Implementations/MuJoCo/Testing/C_L_RoboLoco_Learning.py
new file mode 100644
index 0000000..aa298ff
--- /dev/null
+++ b/Modern Implementations/MuJoCo/Testing/C_L_RoboLoco_Learning.py	
@@ -0,0 +1,166 @@
+#The main workflow of the CartPole example: instantiate env(task(environment+agent models))
+#Composer
+#1.Define agent
+#2.Define environment
+#3.Define task
+#Learning
+#1.Define learning network
+#2.Define net update
+#3.Define learning flow
+###########################
+import numpy as np
+###########################
+import torch.optim as optim
+###########################
+from dm_control import composer
+from dm_control import mujoco # The basic mujoco wrapper.
+from dm_control.mujoco.wrapper.mjbindings import enums # Access to enums and MuJoCo library functions.
+###########################
+from C_L_RoboLoco_Agent import RoboLoco
+from C_L_RoboLoco_Task_Env import PressWithSpecificForce
+from DQN_Classes import ReplayBuffer
+from DQN_Classes import DQNet
+from DQN_Classes import DQNet_update
+from DQN_Classes import epsilon_by_frame
+from DQN_Classes import train_log
+from Matplotlib_Animation import display_video
+###########################
+import wandb
+###########################
+def train_RoboLoco_DQNet(env, model, replay_buffer, batch_size, optimizer, num_frames):
+    wandb.watch(model, log="all", log_freq=10)#"Hooks into the torch model (DQN) to collect gradients and the topology."
+    frames = []
+    losses = []
+    all_rewards = []
+    episode_reward = 0
+    episode_counter = 0
+    time_step = env.reset() #env is the specific environment task defined on the domain.
+    #here specifically it is the task 'balance' defined on domain 'cartpole'.
+    #env.reset() starts a new episode and returns the first 'TimeStep'(step-type, reward, discount, observation).
+    state = time_step.observation #directory of variable values that needs values extracted.
+    positions = [x for x in state['unnamed_model/joint_positions'][0]] # dim-positions = 8
+    velocities = [x for x in state['unnamed_model/joint_velocities'][0]] # dim-velocities = 8
+    state = [*positions, *velocities] #extraction.
+    #defining the project run for loop of frames. Each frame there is: action selection; state transition; observation;
+    #end of episode check; replay buffer push; logging of: rewards (if end of episode), loss (if multiple of a 100th
+    #of the way through the project run); DQN update if there are enough samples in the buffer.
+    for frame_idx in range(1, num_frames + 1):
+        print("frame index:"+str(frame_idx))
+        epsilon = epsilon_by_frame(frame_idx) #value of decaying exponential as function of frame index.
+        action = model.act(state, epsilon) #action selected as function of explore probability epsilon and the state.
+        print(action)
+        _, reward, discount, obs = env.step(action) #action taken. returns step-type, reward, discount, obs.
+        if reward == None: #so that rewards can be summed.
+            reward = 0.0
+        #new state is the extracted and ordered observation values.
+        positions = [x for x in obs['unnamed_model/joint_positions'][0]] # dim-positions = 8
+        velocities = [x for x in obs['unnamed_model/joint_velocities'][0]] # dim-velocities = 8
+        next_state = [*positions, *velocities]  # extraction.
+        done = env._reset_next_step #Boolean variable of whether episode is finished or not.
+        #Currently the episode is not terminating if the pole gets below a certain angle theta threshold.
+        #This must be implemented.
+        replay_buffer.push(state, action, reward, next_state, done) #pushing the sample to the replay buffer.
+
+        #print("discount: "+str(discount)) #1.0
+        #print("done: " + str(done)) #False
+        #print("step-type: "+str(_)) #StepType.MID
+
+        state = next_state #state variable update.
+        episode_reward += reward #adding step rewards to total rewards.
+
+        # frames capture for animation
+        framerate=30
+        if len(frames) < env._physics.data.time * framerate:
+            #data.time resets each episode, so first episode recorded.
+            #can add "+ config["ep_time_limit_secs"]*episode_counter) after data.time to have full frames animation.
+            pixels = env._physics.render(scene_option=scene_option, camera_id='cam0')
+            # rendering the physics model scene into pixels. cameras defined in the cartpole.xml file in suite.
+            frames.append(pixels)  # building list of animation frames.
+        if done:
+            metric_name = "reward"
+            train_log(metric_name, episode_reward, frame_idx) #logging the current total reward by frame index.
+            #state = env.reset() #no reset needed, because done automatically through the 'TimeStep'.
+            all_rewards.append(episode_reward) #tracking the total episode rewards across episodes.
+            #only useful if we get multiple episodes per run.
+            episode_reward = 0
+            episode_counter += 1
+        if len(replay_buffer) > batch_size:
+            loss = DQNet_update(model, optimizer, replay_buffer, batch_size) #calculating the loss on a batch
+            #of data from the replay buffer according to the DQN update function in 'DQN_Classes.py'.
+            losses.append(loss.data) #tracking the loss data for each frame.
+            if frame_idx % (num_frames/100) == 0:
+                metric_name = "loss"
+                train_log(metric_name,loss,frame_idx) #logging the loss data by frame if the frame index is a multiple
+                #of a 100th of the total number of frames in the run.
+    save_name = str(config["env_name"])+"_animation_"+str(config["num_total_frames"])+"_model_frames.mp4"
+    print(save_name)
+    display_video(save_name,frames)
+
+if __name__ == "__main__":
+    #defining the config file which holds defining characteristics about the WandB project run
+    config = {
+        "env_name": "MyRoboLoco",
+        "buffer_size": 100,
+        "batch_size": 24,
+        "num_total_frames": 100,
+        "ep_time_limit_secs": 1
+    }
+    #starts the WandB run. A WandB run is defined by the: project, config directory, job type, etc.
+    run = wandb.init(
+        project="C_L"+config["env_name"],  #name of project on WandB
+        config=config,
+        job_type="learning",
+        #sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics
+        save_code=True,  # optional
+    )
+    ###########################
+    #In my own conception of the dm_control RL pipeline I have split the steps into the Composer step
+    #(defines agent, environment, task in that order) and the Learning step
+    #(defines the agent training procedure on the given task in that environment).
+    #The following are the steps followed when using the Composer dm_control library.
+
+    #Composer
+    #1.Define agent
+    #agent = CartPole()
+    #2.Define environment & 3.Define task
+    #task = CartPoleTask(agent)
+    #env = composer.Environment(task, random_state=np.random.RandomState(42))
+
+    random_state = np.random.RandomState(42)
+    NUM_SUBSTEPS = 25  # The number of physics substeps per control timestep.
+
+    BODY_RADIUS = 0.1
+    BODY_SIZE = (BODY_RADIUS, BODY_RADIUS, BODY_RADIUS / 2)
+    NUM_LEGS = 4
+
+    roboloco = RoboLoco(num_legs=NUM_LEGS, body_size=BODY_SIZE, body_radius=BODY_RADIUS)
+    task = PressWithSpecificForce(roboloco)
+    env = composer.Environment(task, random_state=random_state)
+
+    # Visualize the joint axis
+    scene_option = mujoco.wrapper.core.MjvOption()
+    scene_option.flags[enums.mjtVisFlag.mjVIS_JOINT] = True
+    ###########################
+    #Learning
+    #1.Define learning network
+    num_inputs = 16  #the number of CartPole state variables: (x, v, theta, omega) = (cart+pole pos+vel).
+    num_outputs = 2  #the number of the CartPole action variable options: left and right. A = argmax_a_Q(s,a)
+    #print(len(env.physics.data.qpos)) #2 position variables.
+    #print(len(env.physics.data.qvel)) #2 velocity variables.
+    #print(env.action_spec().shape[0]) #1 action variable (discrete valued: left and right).
+    #defining the agent Q network architecture. Inputs the full state, outputs the action-values Q(s,a).
+    net = DQNet(num_inputs, num_outputs)
+
+    #2.Define net update & 3.Define learning flow
+    optimizer = optim.Adam(net.parameters()) #choosing the optimizer (Adam) which uses the DQN loss gradients
+    #in a particular way to compute backpropagation.
+    buffer_size = config["buffer_size"] #defining the max size of the replay buffer.
+    replay_buffer = ReplayBuffer(buffer_size) #instantiating the replay buffer.
+    batch_size = config["batch_size"] #defining the batch size
+    num_frames = config["num_total_frames"] #defining the run length (total frames over all episodes).
+    #training the agent DQN as a function of: env[domain (agent+environment physics)+task on the domain],
+    #DQN architecture, replay buffer size, batch size, optimizer, total length of run in frames.
+    train_RoboLoco_DQNet(env, net, replay_buffer, batch_size, optimizer, num_frames)
+    ###########################
+    run.finish() #ending the WandB project run. Logging completed.
+    ###########################
\ No newline at end of file
diff --git a/Modern Implementations/MuJoCo/Testing/C_L_RoboLoco_Task_Env.py b/Modern Implementations/MuJoCo/Testing/C_L_RoboLoco_Task_Env.py
new file mode 100644
index 0000000..67904b6
--- /dev/null
+++ b/Modern Implementations/MuJoCo/Testing/C_L_RoboLoco_Task_Env.py	
@@ -0,0 +1,164 @@
+#Defining a task environment for a Robotic Locomotion agent
+import PIL.Image
+import numpy as np
+import matplotlib.pyplot as plt
+
+from dm_control import mjcf
+from dm_control import composer
+from dm_control.composer import variation
+from dm_control.composer.observation import observable
+
+from dm_control.locomotion.arenas import floors
+from dm_control.composer.variation import noises
+from dm_control.composer.variation import distributions
+
+class Button(composer.Entity):
+  """A button Entity which changes colour when pressed with certain force."""
+  def _build(self, target_force_range=(5, 10)):
+    self._min_force, self._max_force = target_force_range
+    self._mjcf_model = mjcf.RootElement()
+    self._geom = self._mjcf_model.worldbody.add(
+        'geom', type='cylinder', size=[0.25, 0.02], rgba=[1, 0, 0, 1])
+    self._site = self._mjcf_model.worldbody.add(
+        'site', type='cylinder', size=self._geom.size*1.01, rgba=[1, 0, 0, 0])
+    self._sensor = self._mjcf_model.sensor.add('touch', site=self._site)
+    self._num_activated_steps = 0
+
+  def _build_observables(self):
+    return ButtonObservables(self)
+
+  @property
+  def mjcf_model(self):
+    return self._mjcf_model
+  # Update the activation (and colour) if the desired force is applied.
+  def _update_activation(self, physics):
+    current_force = physics.bind(self.touch_sensor).sensordata[0]
+    self._is_activated = (current_force >= self._min_force and
+                          current_force <= self._max_force)
+    physics.bind(self._geom).rgba = (
+        [0, 1, 0, 1] if self._is_activated else [1, 0, 0, 1])
+    self._num_activated_steps += int(self._is_activated)
+
+  def initialize_episode(self, physics, random_state):
+    self._reward = 0.0
+    self._num_activated_steps = 0
+    self._update_activation(physics)
+
+  def after_substep(self, physics, random_state):
+    self._update_activation(physics)
+
+  @property
+  def touch_sensor(self):
+    return self._sensor
+
+  @property
+  def num_activated_steps(self):
+    return self._num_activated_steps
+
+
+class ButtonObservables(composer.Observables):
+  """A touch sensor which averages contact force over physics substeps."""
+  @composer.observable
+  def touch_force(self):
+    return observable.MJCFFeature('sensordata', self._entity.touch_sensor,
+                                  buffer_size=NUM_SUBSTEPS, aggregator='mean')
+
+class UniformCircle(variation.Variation):
+  """A uniformly sampled horizontal point on a circle of radius `distance`."""
+  def __init__(self, distance):
+    self._distance = distance
+    self._heading = distributions.Uniform(0, 2*np.pi)
+
+  def __call__(self, initial_value=None, current_value=None, random_state=None):
+    distance, heading = variation.evaluate(
+        (self._distance, self._heading), random_state=random_state)
+    return (distance*np.cos(heading), distance*np.sin(heading), 0)
+
+class PressWithSpecificForce(composer.Task):
+
+  def __init__(self, RoboLoco):
+    self._roboloco = RoboLoco
+    self._arena = floors.Floor()
+    self._arena.add_free_entity(self._roboloco)
+    self._arena.mjcf_model.worldbody.add('light', pos=(0, 0, 4))
+    self._arena.mjcf_model.worldbody.add('camera', name="cam0", pos=(0, -1, .7), xyaxes=(1,0,0,0,1,2))
+    self._button = Button()
+    self._arena.attach(self._button)
+
+    # Configure initial poses
+    self._roboloco_initial_pos = (0, 0, 0.15)
+    self._roboloco_ini_act_pos = (0,0,0,0,0,0,0,0)
+    self._roboloco_ini_vel = (0,0,0)
+    self._roboloco_ini_ang_vel = (0,0,0)
+    button_distance = distributions.Uniform(0.5, .75)
+    self._button_initial_pos = UniformCircle(button_distance)
+
+    # Configure variators
+    self._mjcf_variator = variation.MJCFVariator()
+    self._physics_variator = variation.PhysicsVariator()
+
+    # Configure and enable observables
+    pos_corrptor = noises.Additive(distributions.Normal(scale=0.01))
+    self._roboloco.observables.joint_positions.corruptor = pos_corrptor
+    self._roboloco.observables.joint_positions.enabled = True
+    vel_corruptor = noises.Multiplicative(distributions.LogNormal(sigma=0.01))
+    self._roboloco.observables.joint_velocities.corruptor = vel_corruptor
+    self._roboloco.observables.joint_velocities.enabled = True
+    self._button.observables.touch_force.enabled = True
+
+    def to_button(physics):
+      button_pos, _ = self._button.get_pose(physics)
+      return self._roboloco.global_vector_to_local_frame(physics, button_pos)
+
+    self._task_observables = {}
+    self._task_observables['button_position'] = observable.Generic(to_button)
+
+    for obs in self._task_observables.values():
+      obs.enabled = True
+    self.control_timestep = NUM_SUBSTEPS * self.physics_timestep
+
+  @property
+  def root_entity(self):
+    return self._arena
+
+  @property
+  def task_observables(self):
+    return self._task_observables
+
+  def initialize_episode_mjcf(self, random_state):
+    self._mjcf_variator.apply_variations(random_state)
+
+  def initialize_episode(self, physics, random_state):
+    self._physics_variator.apply_variations(physics, random_state)
+    roboloco_pos, button_pos = variation.evaluate(
+        (self._roboloco_initial_pos, self._button_initial_pos),
+        random_state=random_state)
+    self._roboloco.set_pose(physics, position=roboloco_pos) # randomized
+    self._button.set_pose(physics, position=button_pos) # randomized
+    all_joints = self._roboloco.mjcf_model.find_all('joint', exclude_attachments=False)
+    physics.bind(all_joints).qpos = self._roboloco_ini_act_pos
+    self._roboloco.set_velocity(physics, velocity=self._roboloco_ini_vel, angular_velocity=self._roboloco_ini_ang_vel)
+
+  def get_reward(self, physics):
+    return self._button.num_activated_steps / NUM_SUBSTEPS
+
+global NUM_SUBSTEPS
+NUM_SUBSTEPS = 25 # The number of physics substeps per control timestep.
+
+if __name__ == "__main__":
+    from C_L_RoboLoco_Agent import RoboLoco
+
+    random_state = np.random.RandomState(42)
+
+    BODY_RADIUS = 0.1
+    BODY_SIZE = (BODY_RADIUS, BODY_RADIUS, BODY_RADIUS / 2)
+    NUM_LEGS = 4
+
+    roboloco = RoboLoco(num_legs=NUM_LEGS, body_size=BODY_SIZE, body_radius=BODY_RADIUS)
+    task = PressWithSpecificForce(roboloco)
+    env = composer.Environment(task, random_state=random_state)
+
+    env.reset()
+    img = PIL.Image.fromarray(env.physics.render())
+    plt.imshow(img)
+    print("done")
\ No newline at end of file
diff --git a/Modern Implementations/MuJoCo/Testing/C_L_ddpg.py b/Modern Implementations/MuJoCo/Testing/C_L_ddpg.py
new file mode 100644
index 0000000..e76d460
--- /dev/null
+++ b/Modern Implementations/MuJoCo/Testing/C_L_ddpg.py	
@@ -0,0 +1,145 @@
+import numpy as np
+from copy import deepcopy
+
+import torch.nn as nn
+from torch.optim import Adam
+
+from S_L_CartPole_DDPG_model import (Actor, Critic)
+from C_L_memory import SequentialMemory
+from C_L_random_process import OrnsteinUhlenbeckProcess
+from C_L_util import *
+
+criterion = nn.MSELoss()
+
+class DDPG(object):
+    def __init__(self, num_states, num_actions, args):
+        if args.seed > 0:
+            self.seed(args.seed)
+
+        self.num_states = num_states
+        self.num_actions = num_actions
+
+        net_cfg = {
+            'hidden1':args.hidden1,
+            'hidden2':args.hidden2
+        }
+
+        self.actor = Actor(self.num_states, self.num_actions, **net_cfg)
+        self.actor_target = Actor(self.num_states, self.num_actions, **net_cfg)
+        self.actor_optim = Adam(self.actor.parameters(), lr=args.lrate)
+
+        self.critic = Critic(self.num_states, self.num_actions, **net_cfg)
+        self.critic_target = Critic(self.num_states, self.num_actions, **net_cfg)
+        self.critic_optim = Adam(self.actor.parameters(), lr=args.lrate)
+
+        hard_update(self.actor_target, self.actor)
+        hard_update(self.critic_target, self.critic)
+
+        self.memory = SequentialMemory(limit=args.rmsize, window_length=args.window_length)
+        self.random_process = OrnsteinUhlenbeckProcess(size=num_actions, theta=args.ou_theta, mu=args.ou_mu,
+                                                       sigma=args.ou_sigma)
+
+        self.batch_size = args.batch_size
+        self.tau = args.tau
+        self.d_epsilon = 1.0/args.epsilon
+        self.discount = args.discount
+
+        self.epsilon = 1.0
+        self.s_t = None #most recent state
+        self.a_t = None #most recent action
+        self.is_training = True
+
+    def update_policy(self):
+        # Sample batch
+        state_batch, action_batch, reward_batch, \
+        next_state_batch, terminal_batch = self.memory.sample_and_split(self.batch_size) #using the memory data structures
+        # Prepare for the target q batch
+        t1 = torch.from_numpy(next_state_batch)
+        t1 = t1.float()
+        t2 = self.actor_target(deepcopy(t1))
+        next_q_values = self.critic_target([t1,t2])
+
+        target_q_batch = torch.Tensor(reward_batch) + \
+                         self.discount * torch.Tensor(terminal_batch.astype(float)) * next_q_values
+
+        # Critic update
+        self.critic.zero_grad()
+
+        q_batch = self.critic([torch.Tensor(state_batch), torch.Tensor(action_batch)])
+
+        value_loss = criterion(q_batch, target_q_batch)
+        value_loss.backward()
+        self.critic_optim.step()
+
+        # Actor update
+        self.actor.zero_grad()
+
+        policy_loss = -self.critic([
+            torch.Tensor(state_batch),
+            self.actor(torch.Tensor(state_batch))
+        ])
+
+        policy_loss = policy_loss.mean()
+        policy_loss.backward()
+        self.actor_optim.step()
+
+        # Target update. default tau = 0.001
+        soft_update(self.actor_target, self.actor, self.tau)
+        soft_update(self.critic_target, self.critic, self.tau)
+        return policy_loss
+
+    def eval(self):
+        self.actor.eval()
+        self.actor_target.eval()
+        self.critic.eval()
+        self.critic_target.eval()
+
+    def observe(self,r_t,s_t1,done):
+        if self.is_training:
+            self.memory.append(self.s_t,self.a_t,r_t,done) #note that s_t1 is not included in the memory
+            self.s_t = s_t1
+
+    def random_action(self):
+        action = np.random.uniform(-1.,1.,self.num_actions)
+        self.a_t = action
+        return action
+
+    def select_action(self, s_t, decay_epsilon=True):
+        action = to_numpy(
+            self.actor(torch.Tensor(np.array([s_t])))
+        ).squeeze(0)
+
+        action += self.is_training * max(self.epsilon,0) * self.random_process.sample() #?
+        action = np.clip(action, -1., 1.)
+
+        if decay_epsilon:
+            self.epsilon -= self.d_epsilon
+
+        self.a_t = action
+        return action
+
+    def reset(self,obs):
+        self.s_t = obs
+        self.random_process.reset_states() #?
+
+    def load_weights(self, output):
+        if output is None: return
+        self.actor.load_state_dict(
+            torch.load('{}/actor.pkl'.format(output))
+        )
+        self.critic.load_state_dict(
+            torch.load('{}/critic.pkl'.format(output))
+        )
+
+    def save_model(self,output):
+        torch.save(
+            self.actor.state_dict(),
+            '{}/actor.pkl'.format(output)
+        )
+        torch.save(
+            self.critic.state_dict(),
+            '{}/critic.pkl'.format(output)
+        )
+
+    def seed(self,s):
+        torch.manual_seed(s)
\ No newline at end of file
diff --git a/Modern Implementations/MuJoCo/Testing/C_L_evaluator.py b/Modern Implementations/MuJoCo/Testing/C_L_evaluator.py
new file mode 100644
index 0000000..c14f84e
--- /dev/null
+++ b/Modern Implementations/MuJoCo/Testing/C_L_evaluator.py	
@@ -0,0 +1,78 @@
+#evaluate = Evaluator(args.validate_episodes,
+#        args.validate_steps, args.output, max_episode_length=args.max_episode_length)
+
+import numpy as np
+import matplotlib.pyplot as plt
+from scipy.io import savemat
+
+from C_L_util import *
+
+class Evaluator(object): #typical use is during training to validate the training progress every (x=2000) timesteps.
+    def __init__(self, num_episodes, interval, save_path="", max_episode_length=None):
+        self.num_episodes = num_episodes
+        self.max_episode_length = max_episode_length
+        self.interval = interval
+        self.save_path = save_path
+        self.results = np.array([]).reshape(num_episodes, 0)
+
+    def __call__(self, env, policy, debug=False, visualize=False, save=False):
+        self.is_training = False
+        state = None
+        result = []
+
+        for episode in range(self.num_episodes): # num episodes per validate experiment for good stat. representation.
+
+            # reset at the start of episode
+            time_step = env.reset()
+            state = time_step.observation
+            positions = [x for x in state['unnamed_model/joint_positions'][0]]  # dim-positions = 8
+            velocities = [x for x in state['unnamed_model/joint_velocities'][0]]  # dim-velocities = 8
+            goal = env._task.task_observables['button_position']._raw_callable(env._physics)[:2]  # dim-rel_goal = 2
+            state = [*positions, *velocities, *goal]  # extraction.
+            episode_steps = 0
+            episode_reward = 0.
+
+            assert state is not None
+
+            # start episode
+            done = False
+            while not done:
+                # basic operation, action ,reward, blablabla ...
+                action = policy(state)
+
+                step_type, reward, discount, state = env.step(action)
+                positions = [x for x in state['unnamed_model/joint_positions'][0]]  # dim-positions = 8
+                velocities = [x for x in state['unnamed_model/joint_velocities'][0]]  # dim-velocities = 8
+                goal = env._task.task_observables['button_position']._raw_callable(env._physics)[:2]  # dim-rel_goal = 2
+                state = [*positions, *velocities, *goal]  # extraction.
+                if self.max_episode_length and episode_steps >= self.max_episode_length - 1:
+                    done = True
+
+                if visualize:
+                    env.render(mode='human')
+
+                # update
+                episode_reward += reward
+                episode_steps += 1
+
+            if debug: prYellow('[Evaluate] #Episode{}: episode_reward:{}'.format(episode, episode_reward))
+            result.append(episode_reward)
+
+        result = np.array(result).reshape(-1, 1)
+        self.results = np.hstack([self.results, result])
+
+        if save:
+            self.save_results('{}/validate_reward'.format(self.save_path))
+        return np.mean(result)
+
+    def save_results(self, fn):
+        y = np.mean(self.results, axis=0)
+        error = np.std(self.results, axis=0)
+
+        x = range(0, self.results.shape[1] * self.interval, self.interval)
+        fig, ax = plt.subplots(1, 1, figsize=(6, 5))
+        plt.xlabel('Timestep')
+        plt.ylabel('Average Reward')
+        ax.errorbar(x, y, yerr=error, fmt='-o')
+        plt.savefig(fn + '.png')
+        savemat(fn + '.mat', {'reward': self.results})
\ No newline at end of file
diff --git a/Modern Implementations/MuJoCo/Testing/C_L_main.py b/Modern Implementations/MuJoCo/Testing/C_L_main.py
new file mode 100644
index 0000000..6eaed97
--- /dev/null
+++ b/Modern Implementations/MuJoCo/Testing/C_L_main.py	
@@ -0,0 +1,247 @@
+###########################
+import numpy as np
+import argparse
+from copy import deepcopy
+###########################
+from dm_control import composer
+from dm_control import mujoco # The basic mujoco wrapper.
+from dm_control.mujoco.wrapper.mjbindings import enums # Access to enums and MuJoCo library functions.
+###########################
+from C_L_normalized_env import NormalizedEnv
+from C_L_RoboLoco_Agent import RoboLoco
+from C_L_RoboLoco_Task_Env import PressWithSpecificForce
+from C_L_evaluator import Evaluator
+from C_L_ddpg import DDPG
+from C_L_util import *
+from DQN_Classes import train_log
+from Matplotlib_Animation import display_video
+###########################
+import wandb
+###########################
+def train(num_iterations, agent, env, env_name, evaluate, validate_steps, output, framerate, use_wandb, make_animation,
+          max_episode_length=None, debug=False):
+    ######### FOR WANDB AND ANIMATIONS ###########
+    if use_wandb:
+        wandb.watch((agent.actor,agent.critic), log="all", log_freq=10) # "Hooks into the torch model (DQN) to collect gradients and the topology."
+    frames = episode_frames = []
+    losses = []
+    all_rewards = []
+    ##############################################
+    agent.is_training = True
+    step = episode = episode_steps = 0
+    episode_reward = 0.
+    state = None
+    while step < num_iterations: # total number of training iterations/steps
+        if step % int(num_iterations/min(num_iterations,max_episode_length)) == 0:
+            print("step: "+str(step))
+        # reset if it is the start of episode
+        if state is None:
+            time_step = env.reset()
+            state = time_step.observation
+            state = deepcopy(state)
+            positions = [x for x in state['unnamed_model/joint_positions'][0]]  # dim-positions = 8
+            velocities = [x for x in state['unnamed_model/joint_velocities'][0]]  # dim-velocities = 8
+            goal = env._task.task_observables['button_position']._raw_callable(env._physics)[:2] # dim-rel_goal = 2
+            state = [*positions, *velocities, *goal]  # extraction.
+
+            agent.reset(state)
+
+        # agent pick action ...
+        if step <= args.warmup:
+            action = agent.random_action()
+        else:
+            action = agent.select_action(state) # all necessary exploring behaviour is in here
+
+        # env response with next_observation, reward, terminate_info
+        step_type, reward, discount, state2 = env.step(action)
+        positions = [x for x in state2['unnamed_model/joint_positions'][0]]  # dim-positions = 8
+        velocities = [x for x in state2['unnamed_model/joint_velocities'][0]]  # dim-velocities = 8
+        goal = env._task.task_observables['button_position']._raw_callable(env._physics)[:2]  # dim-rel_goal = 2
+        state2 = [*positions, *velocities, *goal]  # extraction.
+        if reward == None:  # so that rewards can be summed.
+            reward = 0.0
+        done = env._reset_next_step
+        state2 = deepcopy(state2)
+        if max_episode_length and episode_steps >= max_episode_length - 1:
+            done = True
+
+        # agent observe and update policy, and WandB loss logging
+        agent.observe(reward, state2, done)
+        if step > args.warmup:
+            policy_loss = agent.update_policy() # calculating the loss on a batch of data
+            losses.append(policy_loss.data)  # tracking the loss data for each frame.
+            if use_wandb and step % (num_iterations/max_episode_length) == 0:
+                metric_name = "loss"
+                train_log(metric_name, policy_loss, step)
+
+        # [optional] evaluate.
+        if evaluate is not None and validate_steps > 0 and step % validate_steps == 0:
+            policy = lambda x: agent.select_action(x, decay_epsilon=False)
+            validate_reward = evaluate(env, policy, debug=False, visualize=False)
+            if use_wandb:
+                metric_name = "validate_reward"
+                train_log(metric_name, validate_reward, step)
+            if debug: prYellow('[Evaluate] Step_{:07d}: mean_reward:{}'.format(step, validate_reward))
+
+        # [optional] every 10th episode add all frames of the episode to the animation.
+        if make_animation and episode % 10 == 0:
+            if len(episode_frames) < env._physics.time() * framerate:
+                # print("episode: "+str(episode))
+                # print("len_frames: "+str(len(episode_frames)))
+                # print(env._physics.time())
+                # expected episode frames + previous episode frames
+                # data.time resets each episode, so first frame should be recorded.
+                pixels = env._physics.render(scene_option=scene_option, camera_id='cam0')
+                # rendering the physics model scene into pixels. cameras defined in the cartpole.xml file in suite.
+                episode_frames.append(pixels)  # building list of animation frames.
+                # as soon as the 10th episode finishes, extend frames with the episode frames once, and reset.
+                if done:
+                    frames.extend(episode_frames)
+                    episode_frames = []
+
+        # [optional] save intermediate model.
+        if step % int(num_iterations / 3) == 0:
+            agent.save_model(output)
+
+        # update at end of step.
+        step += 1 # total steps
+        episode_steps += 1 # episode steps
+        episode_reward += reward # episode reward
+        state = deepcopy(state2) # change default state
+
+        if done:  # end of episode
+            if debug: prGreen('#{}: episode_reward:{} steps:{}'.format(episode, episode_reward, step))
+            all_rewards.append(episode_reward)  # tracking the total episode rewards across episodes.
+
+            # WandB reward logging
+            if use_wandb:
+                metric_name = "reward"
+                train_log(metric_name, episode_reward, step)  # logging the current total reward by frame index.
+
+            agent.memory.append(
+                state,
+                agent.select_action(state),
+                0., False
+            )
+
+            # reset
+            state = None # calls for state reset
+            episode_steps = 0 # reset episode steps counter
+            episode_reward = 0. # reset episode reward counter
+            episode += 1 # add to episode counter
+
+    if make_animation:
+        save_name = str(env_name) + "-" + str(num_iterations) + "_training_steps.mp4"
+        print(save_name)
+        display_video(save_name, frames)
+
+
+def test(num_episodes, agent, env, evaluate, model_path, visualize=True, debug=False):
+
+    agent.load_weights(model_path)
+    agent.is_training = False
+    agent.eval() # puts agent DDPG layers in evaluation mode (changes behaviour of Dropout/BatchNorm layers if existing)
+    policy = lambda x: agent.select_action(x, decay_epsilon=False)
+
+    for i in range(num_episodes):
+        validate_reward = evaluate(env, policy, debug=debug, visualize=visualize, save=False)
+        if debug: prYellow('[Evaluate] #{}: mean_reward:{}'.format(i, validate_reward))
+
+if __name__ == "__main__":
+
+    parser = argparse.ArgumentParser(description='PyTorch on TORCS with Multi-modal')
+
+    parser.add_argument('--mode', default='train', type=str, help='support option: train/test')
+    parser.add_argument('--env', default='RoboLoco-v0', type=str, help='name of my dm_control composer env')
+    parser.add_argument('--hidden1', default=400, type=int, help='hidden num of first fully connect layer')
+    parser.add_argument('--hidden2', default=300, type=int, help='hidden num of second fully connect layer')
+    parser.add_argument('--lrate', default=0.001, type=float, help='learning rate')
+    parser.add_argument('--prate', default=0.0001, type=float, help='policy net learning rate (only for DDPG)')
+    parser.add_argument('--warmup', default=20, type=int, help='time without training but only filling the replay memory')
+    parser.add_argument('--discount', default=0.99, type=float, help='')
+    parser.add_argument('--batch_size', default=10, type=int, help='minibatch size')
+    parser.add_argument('--rmsize', default=6000000, type=int, help='memory size')
+    parser.add_argument('--window_length', default=1, type=int, help='')
+    parser.add_argument('--tau', default=0.001, type=float, help='moving average for target network')
+    parser.add_argument('--ou_theta', default=0.15, type=float, help='noise theta')
+    parser.add_argument('--ou_sigma', default=0.2, type=float, help='noise sigma')
+    parser.add_argument('--ou_mu', default=0.0, type=float, help='noise mu')
+    parser.add_argument('--validate_episodes', default=3, type=int, help='how many episode to perform during validate experiment')
+    parser.add_argument('--max_episode_length', default=500, type=int, help='max number of steps in an episode')
+    parser.add_argument('--do_eval', default=False, type=bool, help='if to to perform a validate experiment')
+    parser.add_argument('--validate_steps', default=1000, type=int, help='how many steps to perform a validate experiment')
+    parser.add_argument('--output', default='output', type=str, help='')
+    parser.add_argument('--debug', dest='debug', action='store_true')
+    parser.add_argument('--init_w', default=0.003, type=float, help='')
+    parser.add_argument('--train_iter', default=2000, type=int, help='number of training iterations/timesteps')
+    parser.add_argument('--epsilon', default=50000, type=int, help='linear decay of exploration policy')
+    parser.add_argument('--seed', default=42, type=int, help='')
+    parser.add_argument('--resume', default='default', type=str, help='Resuming model path for testing')
+    parser.add_argument('--body_radius', default=0.1, type=float, help='Robot body radius')
+    parser.add_argument('--num_legs', default=4, type=int, help='Robot number of legs')
+    parser.add_argument('--make_animation', default=True, type=bool, help='whether to make an animation for this run or not')
+    parser.add_argument('--frate', default=30, type=int, help='framerate of animations')
+    parser.add_argument('--wandb', default=False, type=bool, help='whether to invoke WandB for this run or not')
+
+    args = parser.parse_args()
+    args.output = get_output_folder(args.output, args.env)
+    if args.resume == 'default':
+        args.resume = 'output/{}-run0'.format(args.env)
+    ############# WANDB ###############
+    # starts the WandB run. A WandB run is defined by the: project, config directory, job type, etc.
+    if args.wandb:
+        run = wandb.init(
+            project="C_L_" + args.env,  # name of project on WandB
+            config=args,
+            job_type="DDPG_RoboLoco_Robot_Deciding",
+            save_code=True)  # optional
+    ############# ENV #################
+
+    BODY_RADIUS = args.body_radius
+    BODY_SIZE = (BODY_RADIUS, BODY_RADIUS, BODY_RADIUS / 2)
+    NUM_LEGS = args.num_legs
+
+    roboloco = RoboLoco(num_legs=NUM_LEGS, body_size=BODY_SIZE, body_radius=BODY_RADIUS)
+    task = PressWithSpecificForce(roboloco)
+    env = composer.Environment(task, random_state=args.seed)
+
+    # Visualize the joint axis
+    scene_option = mujoco.wrapper.core.MjvOption()
+    scene_option.flags[enums.mjtVisFlag.mjVIS_JOINT] = True
+
+    #env = NormalizedEnv(gym.make(args.env)) <-- important. NormalizedEnv just wraps gym env for normalized actions.
+    #env has a obs=env.reset(), obs,rew,done,info=env.step(action),
+    #env.render(), env.seed(args.seed), dim_states=env.observation_space.shape[0], dim_actions=env.action_space.shape[0].
+
+    ############# AGENT ###############
+
+    if args.seed > 0:
+        np.random.seed(args.seed)
+
+    dim_states_qpos = 8 # roboloco._build_observables().joint_positions # number of state variables (joint positions/velocities) 15
+    dim_states_qvel = 8 # roboloco._build_observables().joint_velocities # number of state variables (joint positions/velocities) 14
+    dim_goals = 2 # relative goal position (goal-current torso position for x and y)
+    dim_states = dim_states_qpos + dim_states_qvel + dim_goals
+    dim_actions = len(roboloco.actuators) # number of action variables (number of model actuators we want to move). Roboloco has 8.
+
+    agent = DDPG(dim_states, dim_actions, args)
+    if args.do_eval:
+        evaluate = Evaluator(args.validate_episodes,
+                         args.validate_steps, args.output, max_episode_length=args.max_episode_length)
+    else:
+        evaluate = None
+
+    if args.mode == 'train':
+        train(args.train_iter, agent, env, args.env, evaluate, args.validate_steps, args.output,
+              args.frate, args.wandb, args.make_animation,
+              max_episode_length=args.max_episode_length, debug=args.debug)
+
+    elif args.mode == 'test':
+        test(args.validate_episodes, agent, env, evaluate, args.resume,
+             visualize=True, debug=args.debug)
+
+    else:
+        raise RuntimeError('undefined mode {}'.format(args.mode))
+
+    if args.wandb:
+        run.finish()
\ No newline at end of file
diff --git a/Modern Implementations/MuJoCo/Testing/C_L_memory.py b/Modern Implementations/MuJoCo/Testing/C_L_memory.py
new file mode 100644
index 0000000..1c1a8b2
--- /dev/null
+++ b/Modern Implementations/MuJoCo/Testing/C_L_memory.py	
@@ -0,0 +1,262 @@
+from __future__ import absolute_import
+from collections import deque, namedtuple
+import warnings
+import random
+
+import numpy as np
+
+# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/memory.py
+
+# This is to be understood as a transition: Given `state0`, performing `action`
+# yields `reward` and results in `state1`, which might be `terminal`.
+Experience = namedtuple('Experience', 'state0, action, reward, state1, terminal1')
+
+def sample_batch_indexes(low, high, size):
+    if high - low >= size:
+        # We have enough data. Draw without replacement, that is each index is unique in the
+        # batch. We cannot use `np.random.choice` here because it is horribly inefficient as
+        # the memory grows. See https://github.com/numpy/numpy/issues/2764 for a discussion.
+        # `random.sample` does the same thing (drawing without replacement) and is way faster.
+        r = range(low, high)
+        batch_idxs = random.sample(r, size)
+    else:
+        # Not enough data. Help ourselves with sampling from the range, but the same index
+        # can occur multiple times. This is not good and should be avoided by picking a
+        # large enough warm-up phase.
+        warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')
+        batch_idxs = np.random.random_integers(low, high - 1, size=size)
+    assert len(batch_idxs) == size
+    return batch_idxs
+
+class RingBuffer(object):
+    def __init__(self, maxlen):
+        self.maxlen = maxlen
+        self.start = 0
+        self.length = 0
+        self.data = [None for _ in range(maxlen)]
+
+    def __len__(self):
+        return self.length
+
+    def __getitem__(self, idx):
+        if idx < 0 or idx >= self.length:
+            raise KeyError()
+        return self.data[(self.start + idx) % self.maxlen]
+
+    def append(self, v):
+        if self.length < self.maxlen:
+            # We have space, simply increase the length.
+            self.length += 1
+        elif self.length == self.maxlen:
+            # No space, "remove" the first item.
+            self.start = (self.start + 1) % self.maxlen
+        else:
+            # This should never happen.
+            raise RuntimeError()
+        self.data[(self.start + self.length - 1) % self.maxlen] = v
+
+
+def zeroed_observation(observation):
+    if hasattr(observation, 'shape'):
+        return np.zeros(observation.shape)
+    elif hasattr(observation, '__iter__'):
+        out = []
+        for x in observation:
+            out.append(zeroed_observation(x))
+        return out
+    else:
+        return 0.
+
+
+class Memory(object):
+    def __init__(self, window_length, ignore_episode_boundaries=False):
+        self.window_length = window_length
+        self.ignore_episode_boundaries = ignore_episode_boundaries
+
+        self.recent_observations = deque(maxlen=window_length)
+        self.recent_terminals = deque(maxlen=window_length)
+
+    def sample(self, batch_size, batch_idxs=None):
+        raise NotImplementedError()
+
+    def append(self, observation, action, reward, terminal, training=True):
+        self.recent_observations.append(observation)
+        self.recent_terminals.append(terminal)
+
+    def get_recent_state(self, current_observation):
+        # This code is slightly complicated by the fact that subsequent observations might be
+        # from different episodes. We ensure that an experience never spans multiple episodes.
+        # This is probably not that important in practice but it seems cleaner.
+        state = [current_observation]
+        idx = len(self.recent_observations) - 1
+        for offset in range(0, self.window_length - 1):
+            current_idx = idx - offset
+            current_terminal = self.recent_terminals[current_idx - 1] if current_idx - 1 >= 0 else False
+            if current_idx < 0 or (not self.ignore_episode_boundaries and current_terminal):
+                # The previously handled observation was terminal, don't add the current one.
+                # Otherwise we would leak into a different episode.
+                break
+            state.insert(0, self.recent_observations[current_idx])
+        while len(state) < self.window_length:
+            state.insert(0, zeroed_observation(state[0]))
+        return state
+
+    def get_config(self):
+        config = {
+            'window_length': self.window_length,
+            'ignore_episode_boundaries': self.ignore_episode_boundaries,
+        }
+        return config
+
+
+class SequentialMemory(Memory):
+    def __init__(self, limit, **kwargs):
+        super(SequentialMemory, self).__init__(**kwargs)
+
+        self.limit = limit
+
+        # Do not use deque to implement the memory. This data structure may seem convenient but
+        # it is way too slow on random access. Instead, we use our own ring buffer implementation.
+        self.actions = RingBuffer(limit)
+        self.rewards = RingBuffer(limit)
+        self.terminals = RingBuffer(limit)
+        self.observations = RingBuffer(limit)
+
+    def sample(self, batch_size, batch_idxs=None):
+        if batch_idxs is None:
+            # Draw random indexes such that we have at least a single entry before each
+            # index.
+            batch_idxs = sample_batch_indexes(0, self.nb_entries - 1, size=batch_size)
+        batch_idxs = np.array(batch_idxs) + 1
+        assert np.min(batch_idxs) >= 1
+        assert np.max(batch_idxs) < self.nb_entries
+        assert len(batch_idxs) == batch_size
+
+        # Create experiences
+        experiences = []
+        for idx in batch_idxs:
+            terminal0 = self.terminals[idx - 2] if idx >= 2 else False
+            while terminal0:
+                # Skip this transition because the environment was reset here. Select a new, random
+                # transition and use this instead. This may cause the batch to contain the same
+                # transition twice.
+                idx = sample_batch_indexes(1, self.nb_entries, size=1)[0]
+                terminal0 = self.terminals[idx - 2] if idx >= 2 else False
+            assert 1 <= idx < self.nb_entries
+
+            # This code is slightly complicated by the fact that subsequent observations might be
+            # from different episodes. We ensure that an experience never spans multiple episodes.
+            # This is probably not that important in practice but it seems cleaner.
+            state0 = [self.observations[idx - 1]]
+            for offset in range(0, self.window_length - 1):
+                current_idx = idx - 2 - offset
+                current_terminal = self.terminals[current_idx - 1] if current_idx - 1 > 0 else False
+                if current_idx < 0 or (not self.ignore_episode_boundaries and current_terminal):
+                    # The previously handled observation was terminal, don't add the current one.
+                    # Otherwise we would leak into a different episode.
+                    break
+                state0.insert(0, self.observations[current_idx])
+            while len(state0) < self.window_length:
+                state0.insert(0, zeroed_observation(state0[0]))
+            action = self.actions[idx - 1]
+            reward = self.rewards[idx - 1]
+            terminal1 = self.terminals[idx - 1]
+
+            # Okay, now we need to create the follow-up state. This is state0 shifted on timestep
+            # to the right. Again, we need to be careful to not include an observation from the next
+            # episode if the last state is terminal.
+            state1 = [np.copy(x) for x in state0[1:]]
+            state1.append(self.observations[idx])
+
+            assert len(state0) == self.window_length
+            assert len(state1) == len(state0)
+            experiences.append(Experience(state0=state0, action=action, reward=reward,
+                                          state1=state1, terminal1=terminal1))
+        assert len(experiences) == batch_size
+        return experiences
+
+    def sample_and_split(self, batch_size, batch_idxs=None):
+        experiences = self.sample(batch_size, batch_idxs)
+
+        state0_batch = []
+        reward_batch = []
+        action_batch = []
+        terminal1_batch = []
+        state1_batch = []
+        for e in experiences:
+            state0_batch.append(e.state0)
+            state1_batch.append(e.state1)
+            reward_batch.append(e.reward)
+            action_batch.append(e.action)
+            terminal1_batch.append(0. if e.terminal1 else 1.)
+
+        # Prepare and validate parameters.
+        state0_batch = np.array(state0_batch).reshape(batch_size, -1)
+        state1_batch = np.array(state1_batch).reshape(batch_size, -1)
+        terminal1_batch = np.array(terminal1_batch).reshape(batch_size, -1)
+        reward_batch = np.array(reward_batch).reshape(batch_size, -1)
+        action_batch = np.array(action_batch).reshape(batch_size, -1)
+
+        return state0_batch, action_batch, reward_batch, state1_batch, terminal1_batch
+
+    def append(self, observation, action, reward, terminal, training=True):
+        super(SequentialMemory, self).append(observation, action, reward, terminal, training=training)
+
+        # This needs to be understood as follows: in `observation`, take `action`, obtain `reward`
+        # and weather the next state is `terminal` or not.
+        if training:
+            self.observations.append(observation)
+            self.actions.append(action)
+            self.rewards.append(reward)
+            self.terminals.append(terminal)
+
+    @property
+    def nb_entries(self):
+        return len(self.observations)
+
+    def get_config(self):
+        config = super(SequentialMemory, self).get_config()
+        config['limit'] = self.limit
+        return config
+
+
+class EpisodeParameterMemory(Memory):
+    def __init__(self, limit, **kwargs):
+        super(EpisodeParameterMemory, self).__init__(**kwargs)
+        self.limit = limit
+
+        self.params = RingBuffer(limit)
+        self.intermediate_rewards = []
+        self.total_rewards = RingBuffer(limit)
+
+    def sample(self, batch_size, batch_idxs=None):
+        if batch_idxs is None:
+            batch_idxs = sample_batch_indexes(0, self.nb_entries, size=batch_size)
+        assert len(batch_idxs) == batch_size
+
+        batch_params = []
+        batch_total_rewards = []
+        for idx in batch_idxs:
+            batch_params.append(self.params[idx])
+            batch_total_rewards.append(self.total_rewards[idx])
+        return batch_params, batch_total_rewards
+
+    def append(self, observation, action, reward, terminal, training=True):
+        super(EpisodeParameterMemory, self).append(observation, action, reward, terminal, training=training)
+        if training:
+            self.intermediate_rewards.append(reward)
+
+    def finalize_episode(self, params):
+        total_reward = sum(self.intermediate_rewards)
+        self.total_rewards.append(total_reward)
+        self.params.append(params)
+        self.intermediate_rewards = []
+
+    @property
+    def nb_entries(self):
+        return len(self.total_rewards)
+
+    def get_config(self):
+        config = super(SequentialMemory, self).get_config()
+        config['limit'] = self.limit
+        return
\ No newline at end of file
diff --git a/Modern Implementations/MuJoCo/Testing/C_L_model.py b/Modern Implementations/MuJoCo/Testing/C_L_model.py
new file mode 100644
index 0000000..0f2dfd9
--- /dev/null
+++ b/Modern Implementations/MuJoCo/Testing/C_L_model.py	
@@ -0,0 +1,41 @@
+import numpy as np
+
+import torch
+from torch import nn
+
+class Actor(nn.Module):
+    def __init__(self, num_states, num_actions, hidden1=400, hidden2=300):
+        super(Actor, self).__init__()
+        self.fc1 = nn.Linear(num_states, hidden1)
+        self.fc2 = nn.Linear(hidden1, hidden2)
+        self.fc3 = nn.Linear(hidden2, num_actions)
+        self.relu = nn.ReLU()
+        self.tanh = nn.Tanh()
+
+    def forward(self,x):
+        out = self.fc1(x)
+        out = self.relu(out)
+        out = self.fc2(out)
+        out = self.relu(out)
+        out = self.fc3(out)
+        out = self.tanh(out)
+        #print(out)
+        return out
+
+
+class Critic(nn.Module):
+    def __init__(self, num_states, num_actions, hidden1=400, hidden2=300):
+        super(Critic, self).__init__()
+        self.fc1 = nn.Linear(num_states, hidden1)
+        self.fc2 = nn.Linear(hidden1+num_actions, hidden2)
+        self.fc3 = nn.Linear(hidden2, 1)
+        self.relu = nn.ReLU()
+
+    def forward(self,xa):
+        x,a = xa
+        out = self.fc1(x)
+        out = self.relu(out)
+        out = self.fc2(torch.cat([out,a],1))
+        out = self.relu(out)
+        out = self.fc3(out)
+        return out
\ No newline at end of file
diff --git a/Modern Implementations/MuJoCo/Testing/C_L_normalized_env.py b/Modern Implementations/MuJoCo/Testing/C_L_normalized_env.py
new file mode 100644
index 0000000..6dc71b5
--- /dev/null
+++ b/Modern Implementations/MuJoCo/Testing/C_L_normalized_env.py	
@@ -0,0 +1,16 @@
+import gym
+
+class NormalizedEnv(gym.ActionWrapper):
+    """ Wrap action """
+    #def __init__(self, env):
+        #super().__init__(env):
+
+    def _action(self, action):
+        act_k = (self.action_space.high - self.action_space.low)/ 2.
+        act_b = (self.action_space.high + self.action_space.low)/ 2.
+        return act_k * action + act_b
+
+    def _reverse_action(self, action):
+        act_k_inv = 2./(self.action_space.high - self.action_space.low)
+        act_b = (self.action_space.high + self.action_space.low)/ 2.
+        return act_k_inv * (action - act_b)
\ No newline at end of file
diff --git a/Modern Implementations/MuJoCo/Testing/C_L_random_process.py b/Modern Implementations/MuJoCo/Testing/C_L_random_process.py
new file mode 100644
index 0000000..d1f4359
--- /dev/null
+++ b/Modern Implementations/MuJoCo/Testing/C_L_random_process.py	
@@ -0,0 +1,51 @@
+import numpy as np
+
+
+# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/random.py
+
+class RandomProcess(object):
+    def reset_states(self):
+        pass
+
+
+class AnnealedGaussianProcess(RandomProcess):
+    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):
+        self.mu = mu
+        self.sigma = sigma
+        self.n_steps = 0
+
+        if sigma_min is not None:
+            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)
+            self.c = sigma
+            self.sigma_min = sigma_min
+        else:
+            self.m = 0.
+            self.c = sigma
+            self.sigma_min = sigma
+
+    @property
+    def current_sigma(self):
+        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)
+        return sigma
+
+
+# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab
+class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):
+    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):
+        super().__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)
+        self.theta = theta
+        self.mu = mu
+        self.dt = dt
+        self.x0 = x0
+        self.size = size
+        self.reset_states()
+
+    def sample(self):
+        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(
+            self.dt) * np.random.normal(size=self.size)
+        self.x_prev = x
+        self.n_steps += 1
+        return x
+
+    def reset_states(self):
+        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)
diff --git a/Modern Implementations/MuJoCo/Testing/C_L_util.py b/Modern Implementations/MuJoCo/Testing/C_L_util.py
new file mode 100644
index 0000000..d50c0e7
--- /dev/null
+++ b/Modern Implementations/MuJoCo/Testing/C_L_util.py	
@@ -0,0 +1,43 @@
+import os
+import torch
+
+def prRed(prt): print("\033[91m {}\033[00m" .format(prt))
+def prGreen(prt): print("\033[92m {}\033[00m" .format(prt)) #episode reward
+def prYellow(prt): print("\033[93m {}\033[00m" .format(prt)) #evaluate
+def prLightPurple(prt): print("\033[94m {}\033[00m" .format(prt))
+def prPurple(prt): print("\033[95m {}\033[00m" .format(prt))
+def prCyan(prt): print("\033[96m {}\033[00m" .format(prt))
+def prLightGray(prt): print("\033[97m {}\033[00m" .format(prt))
+def prBlack(prt): print("\033[98m {}\033[00m" .format(prt))
+
+def to_numpy(tensor):
+    return tensor.detach().numpy() #same storage
+
+def hard_update(target, source):
+    for target_param, source_param in zip(target.parameters(),source.parameters()):
+        target_param.data.copy_(source_param.data)
+
+def soft_update(target, source, tau): #get some portion of policy data tau and 1-tau portion of current target data
+    for target_param, param in zip(target.parameters(), source.parameters()):
+        target_param.data.copy_(
+            target_param.data * (1.0 - tau) + param.data * tau
+        )
+
+def get_output_folder(parent_dir, env_name):
+    os.makedirs(parent_dir, exist_ok=True)
+    experiment_id = 0
+    for folder_name in os.listdir(parent_dir):
+        if not os.path.isdir(os.path.join(parent_dir, folder_name)):
+            continue #go back to loop expression
+        try:
+            folder_name = int(folder_name.split('-run')[-1])
+            if folder_name > experiment_id:
+                experiment_id = folder_name
+        except:
+            pass
+    experiment_id += 1
+
+    parent_dir = os.path.join(parent_dir, env_name)
+    parent_dir = parent_dir + '-run{}'.format(experiment_id)
+    os.makedirs(parent_dir, exist_ok=True)
+    return parent_dir
\ No newline at end of file
diff --git a/Modern Implementations/MuJoCo/Testing/DQN_Classes.py b/Modern Implementations/MuJoCo/Testing/DQN_Classes.py
new file mode 100644
index 0000000..0b9c9b9
--- /dev/null
+++ b/Modern Implementations/MuJoCo/Testing/DQN_Classes.py	
@@ -0,0 +1,108 @@
+#Defining the DQN architecture and update function
+###########################
+import random
+import numpy as np
+from collections import deque
+###########################
+import torch
+import torch.nn as nn
+from torch.autograd import Variable
+###########################
+import wandb
+###########################
+#Replay buffer class, initially used for the Cartpole domain. It can be used generally though.
+#Just need state, action, reward, next_state, done variables coming to it,
+#and capacity variable for maximum size limit of the replay buffer deque.
+class ReplayBuffer(object):
+    def __init__(self, capacity):
+        self.capacity = capacity
+        self.buffer = deque(maxlen=capacity)
+
+    def push(self, state, action, reward, next_state, done):
+        state = np.expand_dims(state, 0) #tuple of shape (4,) -> ndarray of shape (1,4): e.g. array([[1,2,3,4]]).
+        next_state = np.expand_dims(next_state, 0)
+        self.buffer.append((state, action, reward, next_state, done))
+
+    def sample(self, batch_size):
+        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))
+        #zip(*...) performs an un-zip function so that a random batch is split into its original sample components.
+        #N.B. each part has the length of the batch size.
+        #May need to check these sizes and shapes if there are more problems beyond episodes not completing
+        #when they should be.
+        return np.concatenate(state), action, reward, np.concatenate(next_state), done
+
+    def __len__(self):
+        return len(self.buffer)
+
+def epsilon_by_frame(frame_idx): #decaying exponential as function of frame index. Explore chance goes from 1.0->0.01.
+    epsilon_start = 1.0
+    epsilon_final = 0.01
+    epsilon_decay = 500
+    epsilon_by_frame = epsilon_final + (epsilon_start - epsilon_final) * np.exp(
+        -1. * frame_idx / epsilon_decay)
+    return epsilon_by_frame
+
+def train_log(metric_name, metric_val, frame_idx): #general function to log a metric with certain name to WandB
+    #against its frame index.
+    metric_val = float(metric_val)
+    wandb.log({metric_name: metric_val}, step=frame_idx)
+    print("Logging "+str(metric_name)+" of: "+str(metric_val)+", at frame index: "+str(frame_idx)+", to WandB")
+
+class DQNet(nn.Module): #architecture of the DQN class is num_inputs=4 -> 256(ReLU) -> num_outputs=2.
+    def __init__(self, num_inputs, num_outputs):
+        super(DQNet, self).__init__()
+        self.num_outputs = num_outputs
+        self.layers = nn.Sequential(
+            nn.Linear(num_inputs, 256),
+            nn.ReLU(),
+            nn.Linear(256, num_outputs)
+        )
+
+    def forward(self, x):
+        return self.layers(x)
+
+    def act(self, state, epsilon):
+        #state = list of state values e.g. looks like [1,2,3,4]
+        #tensor output, exploiting old action value data.
+        if random.random() > epsilon:
+            state = torch.FloatTensor(state).unsqueeze(0) #torch equivalent of numpy.expand_dims(x,axis).
+            #state now looks like tensor([[1,2,3,4]]).
+            action = self.forward(state).max(1)[1]
+            #mapping state to Q values by 'forward' and then doing max(1) to output (max,max_indicies) for axis=1,
+            #then using [1] to grab the indicies tensor.
+            return int(action[0]) #Grabbing the index in the tensor, and ensuring it is an int.
+        #scalar output, exploring new actions for new action value data.
+        else:
+            return random.randrange(self.num_outputs) #outputs random int in range (0,num_outputs=2) -> 0 or 1.
+
+def DQNet_update(model, optimizer, replay_buffer, batch_size):
+    if batch_size > len(replay_buffer):
+        return
+    state, action, reward, next_state, done = replay_buffer.sample(batch_size) #each is the length of the batch size.
+    #state = batch of states, action = batch of actions, etc.
+    #state is 'numpy.ndarray', action is 'tuple', reward is 'tuple', next_state is 'numpy.ndarray', done is 'tuple'.
+
+    #converting to torch tensors. torch.Tensor is an alias for the default tensor type torch.FloatTensor.
+    #maybe can optimize in different situations by choosing specific tensor types.
+    state = torch.Tensor(state)
+    next_state = torch.Tensor(next_state)
+    action = torch.LongTensor(action) #LongTensor because need ints for 'action.unsqueeze(1)'.
+    reward = torch.Tensor(reward)
+    done = torch.Tensor(done) #transforms booleans to False->0.0 and True->1.0. torch.Size([batch_size])
+
+    q_value = model(state) #torch.Size([batch_size, num_outputs]). convert tensor to float by calling float(tensor).
+
+    q_value = q_value.gather(1, action.unsqueeze(1)).squeeze(1) #q_value: torch.Size([batch_size]).
+    #action.unsqueeze(1): torch.Size([batch_size, 1]).
+    #For gather: output[i][j][k] = input[i][index[i][j][k]][k], if dim == 1 (as here).
+    #Here, i:1-batch_size, j=0, k=None (axis=2 doesn't exist).
+
+    next_q_value = model(next_state).max(1)[0] #grabbing the actual max Q value, not the index of it as before.
+    expected_q_value = reward + 0.99 * next_q_value * (1 - done) #done is batch of False->0.0 and True->1.0.
+    loss = (q_value - expected_q_value.data).pow(2).mean() #MSE in Q values. single float tensor value with grad_fn.
+
+    optimizer.zero_grad() #"Sets the gradients of all optimized 'torch.Tensor' s to zero."
+    loss.backward() #"Computes the sum of gradients of given tensors with respect to [model] graph leaves."
+    optimizer.step() #"Performs a single optimization step (parameter update)."
+    return loss
+    ############
diff --git a/Modern Implementations/MuJoCo/Testing/Matplotlib_Animation.py b/Modern Implementations/MuJoCo/Testing/Matplotlib_Animation.py
new file mode 100644
index 0000000..624d255
--- /dev/null
+++ b/Modern Implementations/MuJoCo/Testing/Matplotlib_Animation.py	
@@ -0,0 +1,79 @@
+# Graphics-related
+import matplotlib.animation as animation
+import matplotlib.pyplot as plt
+# The basic mujoco wrapper.
+from dm_control import mujoco
+# Access to enums and MuJoCo library functions.
+from dm_control.mujoco.wrapper.mjbindings import enums
+
+# function that takes in the complete set of frames with framerate and makes, saves, displays animation.
+def display_video(save_name, frames, framerate=30):
+    height, width, _ = frames[0].shape # finding the height and width of animation figure size from frame size.
+    dpi = 70 # dots per inch (resolution).
+    # orig_backend = matplotlib.get_backend()
+    # matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.
+    fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi) # making size of plot accordingly.
+    # matplotlib.use(orig_backend)  # Switch back to the original backend.
+    ax.set_axis_off() # turn x and y axis off.
+    ax.set_aspect('equal') # same scaling for x and y.
+    ax.set_position([0, 0, 1, 1]) # sets the axes position.
+    im = ax.imshow(frames[0]) # display data as an image, i.e., on a 2D regular raster.
+    # function to update the frame data on the animation figure axes.
+    def update(frame):
+        im.set_data(frame)
+        return [im]
+
+    interval = 1000 / framerate # delay between frames in milliseconds.
+    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,
+                                   interval=interval, blit=True, repeat=False)
+    # "fig: the figure object used to get needed events, such as draw or resize"
+    # "func: The function to call at each frame. The first argument will be the next value in frames."
+    # "frames: Source of data to pass func and each frame of the animation."
+    # "interval: delay between frames in milliseconds."
+    # "blit: Whether blitting is used to optimize drawing."
+    # (Blitting speeds up by rendering all non-changing graphic elements into a background image once.)
+    # "repeat: Whether the animation repeats when the sequence of frames is completed."
+    anim.save(save_name, fps=framerate, extra_args=['-vcodec', 'libx264'])
+    # saving the animation with the filename: save_name, with extra arguments.
+    # plt.show() # this command is not required for some reason.
+    return interval
+
+if __name__ == "__main__":
+
+    #xml physics model
+    swinging_body = """
+    <mujoco>
+      <worldbody>
+        <light name="top" pos="0 0 1"/>
+        <body name="box_and_sphere" euler="0 0 -30">
+          <joint name="swing" type="hinge" axis="1 -1 0" pos="-.2 -.2 -.2"/>
+          <geom name="red_box" type="box" size=".2 .2 .2" rgba="1 0 0 1"/>
+          <geom name="green_sphere" pos=".2 .2 .2" size=".1" rgba="0 1 0 1"/>
+        </body>
+      </worldbody>
+    </mujoco>
+    """
+    # physics model made from xml code
+    physics = mujoco.Physics.from_xml_string(swinging_body)
+
+    duration = 2  # (seconds)
+    framerate = 30  # (Hz)
+
+    # Visualize the joint axis
+    #scene_option = mujoco.wrapper.core.MjvOption()
+    #scene_option.flags[enums.mjtVisFlag.mjVIS_JOINT] = True
+
+    # Simulate and display video.
+    frames = []
+    physics.reset()  # Reset state and time
+    # while the physics model time is less than the specified duration, physics model steps,
+    # then frames added to frames list as framerate requires.
+    while physics.data.time < duration:
+        physics.step()
+
+        if len(frames) < physics.data.time * framerate:
+            pixels = physics.render(scene_option=scene_option) # rendering the physics model scene into pixels.
+            frames.append(pixels) # building list of animation frames.
+    save_name = 'basic_animation.mp4',
+    display_video(save_name, frames, framerate) # inputting the fully collected frames to the animation function.
+
diff --git a/Modern Implementations/MuJoCo/MuJoCo_testing.py b/Modern Implementations/MuJoCo/Testing/MuJoCo_testing.py
similarity index 55%
rename from Modern Implementations/MuJoCo/MuJoCo_testing.py
rename to Modern Implementations/MuJoCo/Testing/MuJoCo_testing.py
index 2f9a2d5..7a42d83 100644
--- a/Modern Implementations/MuJoCo/MuJoCo_testing.py	
+++ b/Modern Implementations/MuJoCo/Testing/MuJoCo_testing.py	
@@ -2,7 +2,7 @@ import distutils.util
 import subprocess
 from dm_control import suite
 
-#@title Other imports and helper functions
+# @title Other imports and helper functions
 
 # # General
 # import copy
@@ -16,16 +16,13 @@ import matplotlib
 import matplotlib.animation as animation
 import matplotlib.pyplot as plt
 from IPython.display import HTML
-import PIL.Image
-# Internal loading of video libraries.
-
-#@title All `dm_control` imports required for this tutorial
 
 # The basic mujoco wrapper.
 from dm_control import mujoco
 
 # Access to enums and MuJoCo library functions.
 from dm_control.mujoco.wrapper.mjbindings import enums
+
 # from dm_control.mujoco.wrapper.mjbindings import mjlib
 
 # # PyMJCF
@@ -55,70 +52,65 @@ from dm_control.mujoco.wrapper.mjbindings import enums
 # # Manipulation
 # from dm_control import manipulation
 
-#@title A static model {vertical-output: true}
+# @title A static model {vertical-output: true}
 
 # Font sizes
-SMALL_SIZE = 8
-MEDIUM_SIZE = 10
-BIGGER_SIZE = 12
-plt.rc('font', size=SMALL_SIZE)          # controls default text sizes
-plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title
-plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels
-plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
-plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
-plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize
-plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title
-
-# Inline video helper function
-if os.environ.get('COLAB_NOTEBOOK_TEST', False):
-  # We skip video generation during tests, as it is quite expensive.
-  display_video = lambda *args, **kwargs: None
-else:
-  def display_video(frames, framerate=30):
+# SMALL_SIZE = 8
+# MEDIUM_SIZE = 10
+# BIGGER_SIZE = 12
+# plt.rc('font', size=SMALL_SIZE)  # controls default text sizes
+# plt.rc('axes', titlesize=SMALL_SIZE)  # fontsize of the axes title
+# plt.rc('axes', labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels
+# plt.rc('xtick', labelsize=SMALL_SIZE)  # fontsize of the tick labels
+# plt.rc('ytick', labelsize=SMALL_SIZE)  # fontsize of the tick labels
+# plt.rc('legend', fontsize=SMALL_SIZE)  # legend fontsize
+# plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title
+
+def display_video(frames, framerate=30):
     height, width, _ = frames[0].shape
     dpi = 70
     orig_backend = matplotlib.get_backend()
-    matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.
+    #matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.
     fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)
     matplotlib.use(orig_backend)  # Switch back to the original backend.
     ax.set_axis_off()
     ax.set_aspect('equal')
     ax.set_position([0, 0, 1, 1])
     im = ax.imshow(frames[0])
+
     def update(frame):
-      im.set_data(frame)
-      return [im]
-    interval = 1000/framerate
+        im.set_data(frame)
+        return [im]
+
+    interval = 1000 / framerate
     anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,
                                    interval=interval, blit=True, repeat=False)
+    anim.save('basic_animation.mp4', fps=30, extra_args=['-vcodec', 'libx264'])
+    plt.show()
     return HTML(anim.to_html5_video())
 
-# Seed numpy's global RNG so that cell outputs are deterministic. We also try to
-# use RandomState instances that are local to a single cell wherever possible.
-np.random.seed(42)
-
-static_model = """
-<mujoco>
-  <worldbody>
-    <light name="top" pos="0 0 1"/>
-    <geom name="red_box" type="box" size=".2 .2 .2" rgba="1 0 0 1"/>
-    <geom name="green_sphere" pos=".2 .2 .2" size=".1" rgba="0 1 0 1"/>
-  </worldbody>
-</mujoco>
-"""
-physics = mujoco.Physics.from_xml_string(static_model)
-pixels = physics.render()
-img = PIL.Image.fromarray(pixels)
-plt.imshow(img)
-plt.show()
-
-#@title A child body with a joint { vertical-output: true }
-
+# static_model = """
+# <mujoco>
+#   <worldbody>
+#     <light name="top" pos="0 0 1"/>
+#     <geom name="red_box" type="box" size=".2 .2 .2" rgba="1 0 0 1"/>
+#     <geom name="green_sphere" pos=".2 .2 .2" size=".1" rgba="0 1 0 1"/>
+#   </worldbody>
+# </mujoco>
+# """
+# physics = mujoco.Physics.from_xml_string(static_model)
+# pixels = physics.render()
+# img = PIL.Image.fromarray(pixels)
+# plt.imshow(img)
+# plt.show()
+#
+# # @title A child body with a joint { vertical-output: true }
+#
 swinging_body = """
 <mujoco>
   <worldbody>
     <light name="top" pos="0 0 1"/>
-    <body name="box_and_sphere" euler="0 0 -30">  
+    <body name="box_and_sphere" euler="0 0 -30">
       <joint name="swing" type="hinge" axis="1 -1 0" pos="-.2 -.2 -.2"/>
       <geom name="red_box" type="box" size=".2 .2 .2" rgba="1 0 0 1"/>
       <geom name="green_sphere" pos=".2 .2 .2" size=".1" rgba="0 1 0 1"/>
@@ -128,14 +120,14 @@ swinging_body = """
 """
 physics = mujoco.Physics.from_xml_string(swinging_body)
 # Visualize the joint axis.
-scene_option = mujoco.wrapper.core.MjvOption()
-scene_option.flags[enums.mjtVisFlag.mjVIS_JOINT] = True
-pixels = physics.render(scene_option=scene_option)
-PIL.Image.fromarray(pixels)
+# scene_option = mujoco.wrapper.core.MjvOption()
+# scene_option.flags[enums.mjtVisFlag.mjVIS_JOINT] = True
+# pixels = physics.render(scene_option=scene_option)
+# PIL.Image.fromarray(pixels)
 
 #@title Making a video {vertical-output: true}
 
-duration = 2    # (seconds)
+duration = 2  # (seconds)
 framerate = 30  # (Hz)
 
 # Visualize the joint axis
@@ -146,8 +138,10 @@ scene_option.flags[enums.mjtVisFlag.mjVIS_JOINT] = True
 frames = []
 physics.reset()  # Reset state and time
 while physics.data.time < duration:
-  physics.step()
-  if len(frames) < physics.data.time * framerate:
-    pixels = physics.render(scene_option=scene_option)
-    frames.append(pixels)
-display_video(frames, framerate)
\ No newline at end of file
+    physics.step()
+
+    if len(frames) < physics.data.time * framerate:
+        pixels = physics.render(scene_option=scene_option)
+        frames.append(pixels)
+display_video(frames, framerate)
+
diff --git a/Modern Implementations/MuJoCo/Testing/S_L_CartPole_DDPG_evaluator.py b/Modern Implementations/MuJoCo/Testing/S_L_CartPole_DDPG_evaluator.py
new file mode 100644
index 0000000..4bc0fb1
--- /dev/null
+++ b/Modern Implementations/MuJoCo/Testing/S_L_CartPole_DDPG_evaluator.py	
@@ -0,0 +1,77 @@
+#evaluate = Evaluator(args.validate_episodes,
+#        args.validate_steps, args.output, max_episode_length=args.max_episode_length)
+
+import numpy as np
+import matplotlib.pyplot as plt
+from scipy.io import savemat
+
+from C_L_util import *
+
+class Evaluator(object): #typical use is during training to validate the training progress every (x=2000) timesteps.
+    def __init__(self, num_episodes, interval, save_path="", max_episode_length=None):
+        self.num_episodes = num_episodes
+        self.max_episode_length = max_episode_length
+        self.interval = interval
+        self.save_path = save_path
+        self.results = np.array([]).reshape(num_episodes, 0)
+
+    def __call__(self, env, policy, debug=False, visualize=False, save=False):
+        self.is_training = False
+        state = None
+        result = []
+
+        for episode in range(self.num_episodes): # num episodes per validate experiment for good stat. representation.
+
+            # reset at the start of episode
+            time_step = env.reset()
+            state = time_step.observation
+            state = ([state['position'][0], state['position'][2], state['velocity'][0], state['velocity'][1]]) #extraction.
+            episode_steps = 0
+            episode_reward = 0.
+
+            assert state is not None
+
+            # start episode
+            done = False
+            while not done:
+                # basic operation, action ,reward, blablabla ...
+                action = policy(state)
+                # interpret continuous action to discrete action-space CartPole environment
+                if action < 0:
+                    action = 0
+                else:
+                    action = 1
+
+                step_type, reward, discount, state = env.step(action)
+                state = ([state['position'][0], state['position'][2], state['velocity'][0], state['velocity'][1]]) #extraction.
+                if self.max_episode_length and episode_steps >= self.max_episode_length - 1:
+                    done = True
+
+                if visualize:
+                    env.render(mode='human')
+
+                # update
+                episode_reward += reward
+                episode_steps += 1
+
+            if debug: prYellow('[Evaluate] #Episode{}: episode_reward:{}'.format(episode, episode_reward))
+            result.append(episode_reward)
+
+        result = np.array(result).reshape(-1, 1)
+        self.results = np.hstack([self.results, result])
+
+        if save:
+            self.save_results('{}/validate_reward'.format(self.save_path))
+        return np.mean(result)
+
+    def save_results(self, fn):
+        y = np.mean(self.results, axis=0)
+        error = np.std(self.results, axis=0)
+
+        x = range(0, self.results.shape[1] * self.interval, self.interval)
+        fig, ax = plt.subplots(1, 1, figsize=(6, 5))
+        plt.xlabel('Timestep')
+        plt.ylabel('Average Reward')
+        ax.errorbar(x, y, yerr=error, fmt='-o')
+        plt.savefig(fn + '.png')
+        savemat(fn + '.mat', {'reward': self.results})
\ No newline at end of file
diff --git a/Modern Implementations/MuJoCo/Testing/S_L_CartPole_DDPG_main.py b/Modern Implementations/MuJoCo/Testing/S_L_CartPole_DDPG_main.py
new file mode 100644
index 0000000..4fb7a65
--- /dev/null
+++ b/Modern Implementations/MuJoCo/Testing/S_L_CartPole_DDPG_main.py	
@@ -0,0 +1,233 @@
+###########################
+import numpy as np
+import argparse
+from copy import deepcopy
+###########################
+from dm_control import suite
+from dm_control import mujoco # The basic mujoco wrapper.
+from dm_control.mujoco.wrapper.mjbindings import enums # Access to enums and MuJoCo library functions.
+###########################
+from S_L_CartPole_DDPG_evaluator import Evaluator
+from C_L_ddpg import DDPG
+from C_L_util import *
+from DQN_Classes import train_log
+from Matplotlib_Animation import display_video
+###########################
+import wandb
+###########################
+def train(num_iterations, agent, env, env_name, evaluate, validate_steps, output, framerate, use_wandb, make_animation,
+          max_episode_length=None, debug=False):
+    ######### FOR WANDB AND ANIMATIONS ###########
+    if use_wandb:
+        wandb.watch((agent.actor,agent.critic), log="all", log_freq=10) # "Hooks into the torch model (DQN) to collect gradients and the topology."
+    frames = episode_frames = []
+    losses = []
+    all_rewards = []
+    ##############################################
+    agent.is_training = True
+    step = episode = episode_steps = 0
+    episode_reward = 0.
+    state = None
+    while step < num_iterations: # total number of training iterations/steps
+        if step % max_episode_length == 0:
+            print("step: "+str(step))
+        # reset if it is the start of episode
+        if state is None:
+            time_step = env.reset()
+            state = deepcopy(time_step.observation)
+            state = ([state['position'][0], state['position'][2], state['velocity'][0], state['velocity'][1]]) #extraction.
+            agent.reset(state)
+
+        # agent pick action ...
+        if step <= args.warmup:
+            action = agent.random_action()
+        else:
+            action = agent.select_action(state) # all necessary exploring behaviour is in here
+        # interpret continuous action to discrete action-space CartPole environment
+        if action < 0:
+            action = 0
+        else:
+            action = 1
+
+        # env response with next_observation, reward, terminate_info
+        step_type, reward, discount, state2 = env.step(action)
+        state2 = deepcopy(state2)
+        state2 = ([state2['position'][0], state2['position'][2], state2['velocity'][0], state2['velocity'][1]])  # extraction.
+        if reward == None:  # so that rewards can be summed.
+            reward = 0.0
+        done = env._reset_next_step
+        if max_episode_length and episode_steps >= max_episode_length - 1:
+            done = True
+
+        # agent observe and update policy, and WandB loss logging
+        agent.observe(reward, state2, done)
+        if step > args.warmup:
+            policy_loss = agent.update_policy() # calculating the loss on a batch of data
+            losses.append(policy_loss.data)  # tracking the loss data for each frame.
+            if use_wandb and step % (num_iterations/max_episode_length) == 0:
+                metric_name = "loss"
+                train_log(metric_name, policy_loss, step)
+
+        # [optional] evaluate.
+        if evaluate is not None and validate_steps > 0 and step % validate_steps == 0:
+            policy = lambda x: agent.select_action(x, decay_epsilon=False)
+            validate_reward = evaluate(env, policy, debug=False, visualize=False)
+            if use_wandb:
+                metric_name = "validate_reward"
+                train_log(metric_name, validate_reward, step)
+            if debug: prYellow('[Evaluate] Step_{:07d}: mean_reward:{}'.format(step, validate_reward))
+
+        # [optional] every 10th episode add all frames of the episode to the animation.
+        if make_animation and episode % 10 == 0:
+            if len(episode_frames) < env._physics.time() * framerate:
+                # print("episode: "+str(episode))
+                # print("len_frames: "+str(len(episode_frames)))
+                # print(env._physics.time())
+                # expected episode frames + previous episode frames
+                # data.time resets each episode, so first frame should be recorded.
+                pixels = env._physics.render(scene_option=scene_option, camera_id='lookatcart')
+                # rendering the physics model scene into pixels. cameras defined in the cartpole.xml file in suite.
+                episode_frames.append(pixels)  # building list of animation frames.
+                # as soon as the 10th episode finishes, extend frames with the episode frames once, and reset.
+                if done:
+                    frames.extend(episode_frames)
+                    episode_frames = []
+
+        # [optional] save intermediate model.
+        if step % int(num_iterations / 3) == 0:
+            agent.save_model(output)
+
+        # update at end of step.
+        step += 1 # total steps
+        episode_steps += 1 # episode steps
+        episode_reward += reward # episode reward
+        state = deepcopy(state2) # change default state
+
+        if done:  # end of episode
+            if debug: prGreen('#{}: episode_reward:{} steps:{}'.format(episode, episode_reward, step))
+            all_rewards.append(episode_reward)  # tracking the total episode rewards across episodes.
+
+            # WandB reward logging
+            if use_wandb:
+                metric_name = "reward"
+                train_log(metric_name, episode_reward, step)  # logging the current total reward by frame index.
+
+            agent.memory.append(
+                state,
+                agent.select_action(state),
+                0., False
+            )
+
+            # reset
+            state = None # calls for state reset
+            episode_steps = 0 # reset episode steps counter
+            episode_reward = 0. # reset episode reward counter
+            episode += 1 # add to episode counter
+
+    if make_animation:
+        save_name = str(env_name) + "-" + str(num_iterations) + "_training_steps.mp4"
+        print(save_name)
+        display_video(save_name, frames)
+
+
+def test(num_episodes, agent, env, evaluate, model_path, visualize=True, debug=False):
+
+    agent.load_weights(model_path)
+    agent.is_training = False
+    agent.eval() # puts agent DDPG layers in evaluation mode (changes behaviour of Dropout/BatchNorm layers if existing)
+    policy = lambda x: agent.select_action(x, decay_epsilon=False)
+
+    for i in range(num_episodes):
+        validate_reward = evaluate(env, policy, debug=debug, visualize=visualize, save=False)
+        if debug: prYellow('[Evaluate] #{}: mean_reward:{}'.format(i, validate_reward))
+
+if __name__ == "__main__":
+
+    parser = argparse.ArgumentParser(description='PyTorch on TORCS with Multi-modal')
+
+    parser.add_argument('--mode', default='train', type=str, help='support option: train/test')
+    parser.add_argument('--env', default='CartPole-v0', type=str, help='name of my dm_control composer env')
+    parser.add_argument('--hidden1', default=400, type=int, help='hidden num of first fully connect layer')
+    parser.add_argument('--hidden2', default=300, type=int, help='hidden num of second fully connect layer')
+    parser.add_argument('--lrate', default=0.001, type=float, help='learning rate')
+    parser.add_argument('--prate', default=0.0001, type=float, help='policy net learning rate (only for DDPG)')
+    parser.add_argument('--warmup', default=20, type=int, help='time without training but only filling the replay memory')
+    parser.add_argument('--discount', default=0.99, type=float, help='')
+    parser.add_argument('--batch_size', default=10, type=int, help='minibatch size')
+    parser.add_argument('--rmsize', default=6000000, type=int, help='memory size')
+    parser.add_argument('--window_length', default=1, type=int, help='')
+    parser.add_argument('--tau', default=0.001, type=float, help='moving average for target network')
+    parser.add_argument('--ou_theta', default=0.15, type=float, help='noise theta')
+    parser.add_argument('--ou_sigma', default=0.2, type=float, help='noise sigma')
+    parser.add_argument('--ou_mu', default=0.0, type=float, help='noise mu')
+    parser.add_argument('--validate_episodes', default=3, type=int, help='how many episode to perform during validate experiment')
+    parser.add_argument('--max_episode_length', default=500, type=int, help='max number of steps in an episode')
+    parser.add_argument('--max_episode_time', default=1, type=int, help='max number of seconds in an episode')
+    parser.add_argument('--do_eval', default=False, type=bool, help='if to to perform a validate experiment')
+    parser.add_argument('--validate_steps', default=1000, type=int, help='how many steps to perform a validate experiment')
+    parser.add_argument('--output', default='output', type=str, help='')
+    parser.add_argument('--debug', dest='debug', action='store_true')
+    parser.add_argument('--init_w', default=0.003, type=float, help='')
+    parser.add_argument('--train_iter', default=2000, type=int, help='number of training iterations/timesteps')
+    parser.add_argument('--epsilon', default=50000, type=int, help='linear decay of exploration policy')
+    parser.add_argument('--seed', default=42, type=int, help='')
+    parser.add_argument('--resume', default='default', type=str, help='Resuming model path for testing')
+    parser.add_argument('--make_animation', default=True, type=bool, help='whether to make an animation for this run or not')
+    parser.add_argument('--frate', default=30, type=int, help='framerate of animations')
+    parser.add_argument('--wandb', default=False, type=bool, help='whether to invoke WandB for this run or not')
+
+    args = parser.parse_args()
+    args.output = get_output_folder(args.output, args.env)
+    if args.resume == 'default':
+        args.resume = 'output/{}-run0'.format(args.env)
+    ############# WANDB ###############
+    # starts the WandB run. A WandB run is defined by the: project, config directory, job type, etc.
+    if args.wandb:
+        run = wandb.init(
+            project="C_L_" + args.env,  # name of project on WandB
+            config=args,
+            job_type="DDPG_CartPole",
+            save_code=True)  # optional
+    ############# ENV #################
+
+    env = suite.load('cartpole', 'balance', task_kwargs={
+        'time_limit': args.max_episode_time,
+        'random': args.seed})
+
+    # Visualize the joint axis
+    scene_option = mujoco.wrapper.core.MjvOption()
+    scene_option.flags[enums.mjtVisFlag.mjVIS_JOINT] = True
+
+    #env = NormalizedEnv(gym.make(args.env)) <-- important. NormalizedEnv just wraps gym env for normalized actions.
+    #env has a obs=env.reset(), obs,rew,done,info=env.step(action),
+    #env.render(), env.seed(args.seed), dim_states=env.observation_space.shape[0], dim_actions=env.action_space.shape[0].
+
+    ############# AGENT ###############
+
+    if args.seed > 0:
+        np.random.seed(args.seed)
+
+    dim_states = 4
+    dim_actions = 1
+
+    agent = DDPG(dim_states, dim_actions, args)
+    if args.do_eval:
+        evaluate = Evaluator(args.validate_episodes,
+                         args.validate_steps, args.output, max_episode_length=args.max_episode_length)
+    else:
+        evaluate = None
+
+    if args.mode == 'train':
+        train(args.train_iter, agent, env, args.env, evaluate, args.validate_steps, args.output,
+              args.frate, args.wandb, args.make_animation,
+              max_episode_length=args.max_episode_length, debug=args.debug)
+
+    elif args.mode == 'test':
+        test(args.validate_episodes, agent, env, evaluate, args.resume,
+             visualize=True, debug=args.debug)
+
+    else:
+        raise RuntimeError('undefined mode {}'.format(args.mode))
+
+    if args.wandb:
+        run.finish()
diff --git a/Modern Implementations/MuJoCo/Testing/S_L_CartPole_DDPG_model.py b/Modern Implementations/MuJoCo/Testing/S_L_CartPole_DDPG_model.py
new file mode 100644
index 0000000..0f2dfd9
--- /dev/null
+++ b/Modern Implementations/MuJoCo/Testing/S_L_CartPole_DDPG_model.py	
@@ -0,0 +1,41 @@
+import numpy as np
+
+import torch
+from torch import nn
+
+class Actor(nn.Module):
+    def __init__(self, num_states, num_actions, hidden1=400, hidden2=300):
+        super(Actor, self).__init__()
+        self.fc1 = nn.Linear(num_states, hidden1)
+        self.fc2 = nn.Linear(hidden1, hidden2)
+        self.fc3 = nn.Linear(hidden2, num_actions)
+        self.relu = nn.ReLU()
+        self.tanh = nn.Tanh()
+
+    def forward(self,x):
+        out = self.fc1(x)
+        out = self.relu(out)
+        out = self.fc2(out)
+        out = self.relu(out)
+        out = self.fc3(out)
+        out = self.tanh(out)
+        #print(out)
+        return out
+
+
+class Critic(nn.Module):
+    def __init__(self, num_states, num_actions, hidden1=400, hidden2=300):
+        super(Critic, self).__init__()
+        self.fc1 = nn.Linear(num_states, hidden1)
+        self.fc2 = nn.Linear(hidden1+num_actions, hidden2)
+        self.fc3 = nn.Linear(hidden2, 1)
+        self.relu = nn.ReLU()
+
+    def forward(self,xa):
+        x,a = xa
+        out = self.fc1(x)
+        out = self.relu(out)
+        out = self.fc2(torch.cat([out,a],1))
+        out = self.relu(out)
+        out = self.fc3(out)
+        return out
\ No newline at end of file
diff --git a/Modern Implementations/MuJoCo/Testing/S_L_CartPole_DQN.py b/Modern Implementations/MuJoCo/Testing/S_L_CartPole_DQN.py
new file mode 100644
index 0000000..07705e7
--- /dev/null
+++ b/Modern Implementations/MuJoCo/Testing/S_L_CartPole_DQN.py	
@@ -0,0 +1,175 @@
+#The main workflow of the CartPole example: instantiate env(task(environment+agent models))
+#Composer
+#1.Define agent
+#2.Define environment
+#3.Define task
+#Learning
+#1.Define learning network
+#2.Define net update
+#3.Define learning flow
+###########################
+import inspect
+import numpy as np
+from dm_control import mujoco
+from dm_control import mjcf
+from dm_control import composer
+from dm_control import suite
+###########################
+#Graphics-related
+import matplotlib.animation as animation
+import matplotlib.pyplot as plt
+#The basic mujoco wrapper.
+from dm_control import mujoco
+#Access to enums and MuJoCo library functions.
+from dm_control.mujoco.wrapper.mjbindings import enums
+import wandb
+###########################
+import torch.optim as optim
+###########################
+from DQN_Classes import ReplayBuffer
+from DQN_Classes import DQNet
+from DQN_Classes import DQNet_update
+from DQN_Classes import epsilon_by_frame
+from DQN_Classes import train_log
+from Matplotlib_Animation import display_video
+###########################
+def train_CartPole_DQNet(env, model, replay_buffer, batch_size, optimizer, num_frames):
+    wandb.watch(model, log="all", log_freq=10)#"Hooks into the torch model (DQN) to collect gradients and the topology."
+    frames = []
+    losses = []
+    all_rewards = []
+    episode_reward = 0
+    episode_counter = 0
+    time_step = env.reset() #env is the specific environment task defined on the domain.
+    #here specifically it is the task 'balance' defined on domain 'cartpole'.
+    #env.reset() starts a new episode and returns the first 'TimeStep'(step-type, reward, discount, observation).
+    state = time_step.observation #directory of variable values that needs values extracted.
+    state = ([state['position'][0], state['position'][2], state['velocity'][0], state['velocity'][1]]) #extraction.
+    #defining the project run for loop of frames. Each frame there is: action selection; state transition; observation;
+    #end of episode check; replay buffer push; logging of: rewards (if end of episode), loss (if multiple of a 100th
+    #of the way through the project run); DQN update if there are enough samples in the buffer.
+    for frame_idx in range(1, num_frames + 1):
+        print("frame index:"+str(frame_idx))
+        epsilon = epsilon_by_frame(frame_idx) #value of decaying exponential as function of frame index.
+        action = model.act(state, epsilon) #action selected as function of explore probability epsilon and the state.
+        #print(action)
+        _, reward, discount, obs = env.step(action) #action taken. returns step-type, reward, discount, obs.
+        if reward == None: #so that rewards can be summed.
+            reward = 0.0
+        #new state is the extracted and ordered observation values.
+        next_state = ([obs['position'][0], obs['position'][2],
+                       obs['velocity'][0], obs['velocity'][1]])
+
+        if abs(next_state[2]) >= 0.5*np.sqrt(3): # if pole gets below 30 degrees from vertical on either side
+            env._reset_next_step = True
+        done = env._reset_next_step  # Boolean variable of whether episode is finished or not.
+        #Currently the episode is not terminating if the pole gets below a certain angle theta threshold.
+        #This must be implemented.
+        replay_buffer.push(state, action, reward, next_state, done) #pushing the sample to the replay buffer.
+
+        #print("discount: "+str(discount)) #1.0
+        #print("done: " + str(done)) #False
+        print("action: " + str(action))
+        print("pole-angle: " + str(next_state[2]))
+        print("step-type: "+str(_)) #StepType.MID
+
+        state = next_state #state variable update.
+        episode_reward += reward #adding step rewards to total rewards.
+
+        # frames capture for animation
+        framerate=30
+        if len(frames) < (env._physics.data.time + config["ep_time_limit_secs"]*episode_counter) * framerate:
+            #data.time resets each episode, so first episode recorded.
+            pixels = env._physics.render(scene_option=scene_option, camera_id='lookatcart')
+            # rendering the physics model scene into pixels. cameras defined in the cartpole.xml file in suite.
+            frames.append(pixels)  # building list of animation frames.
+
+        if done:
+            metric_name = "reward"
+            train_log(metric_name, episode_reward, frame_idx) #logging the current total reward by frame index.
+            #state = env.reset() #no reset needed, because done automatically through the 'TimeStep'.
+            all_rewards.append(episode_reward) #tracking the total episode rewards across episodes.
+            #only useful if we get multiple episodes per run.
+            episode_reward = 0
+            episode_counter += 1
+        if len(replay_buffer) > batch_size:
+            loss = DQNet_update(model, optimizer, replay_buffer, batch_size) #calculating the loss on a batch
+            #of data from the replay buffer according to the DQN update function in 'DQN_Classes.py'.
+            losses.append(loss.data) #tracking the loss data for each frame.
+            if frame_idx % (num_frames/100) == 0:
+                metric_name = "loss"
+                train_log(metric_name,loss,frame_idx) #logging the loss data by frame if the frame index is a multiple
+                #of a 100th of the total number of frames in the run.
+    save_name = str(config["env_name"])+"_animation_"+str(config["num_total_frames"])+"_model_frames.mp4"
+    print(save_name)
+    display_video(save_name,frames,framerate)
+
+if __name__ == "__main__":
+    #defining the config file which holds defining characteristics about the WandB project run
+    config = {
+        "env_name": "MyCartPole",
+        "buffer_size": 100,
+        "batch_size": 24,
+        "num_total_frames": 1000,
+        "ep_time_limit_secs": 1
+    }
+    #starts the WandB run. A WandB run is defined by the: project, config directory, job type, etc.
+    run = wandb.init(
+        project="C_L"+config["env_name"],  #name of project on WandB
+        config=config,
+        job_type="learning",
+        #sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics
+        save_code=True,  # optional
+    )
+    ###########################
+    #In my own conception of the dm_control RL pipeline I have split the steps into the Composer step
+    #(defines agent, environment, task in that order) and the Learning step
+    #(defines the agent training procedure on the given task in that environment).
+    #The following are the steps followed when using the Composer dm_control library.
+
+    #Composer
+    #1.Define agent
+    #agent = CartPole()
+    #2.Define environment & 3.Define task
+    #task = CartPoleTask(agent)
+    #env = composer.Environment(task, random_state=np.random.RandomState(42))
+
+    #np.random.RandomState(x) defines the random number generation seed such that the psuedo-random generated numbers
+    #will be the same each run for the sake of exact comparison between runs.
+    random_state = np.random.RandomState(2)
+    #loads a pre-defined environment constructed from the domain(agent+environment physics models)/task combination
+    #from the dm_control suite library.
+    env = suite.load('cartpole', 'balance', task_kwargs={
+        'time_limit': config["ep_time_limit_secs"],
+        'random': random_state})
+    #suite.load is defined in suite.__init__.py
+    #print(inspect.getmembers(env, inspect.ismethod))
+    #prints methods of env object in list of tuples
+
+    # Visualize the joint axis
+    scene_option = mujoco.wrapper.core.MjvOption()
+    scene_option.flags[enums.mjtVisFlag.mjVIS_JOINT] = True
+    ###########################
+    #Learning
+    #1.Define learning network
+    num_inputs = 4  #the number of CartPole state variables: (x, v, theta, omega) = (cart+pole pos+vel).
+    num_outputs = 2  #the number of the CartPole action variable options: left and right. A = argmax_a_Q(s,a)
+    #print(len(env.physics.data.qpos)) #2 position variables.
+    #print(len(env.physics.data.qvel)) #2 velocity variables.
+    #print(env.action_spec().shape[0]) #1 action variable (discrete valued: left and right).
+    #defining the agent Q network architecture. Inputs the full state, outputs the action-values Q(s,a).
+    net = DQNet(num_inputs, num_outputs)
+
+    #2.Define net update & 3.Define learning flow
+    optimizer = optim.Adam(net.parameters()) #choosing the optimizer (Adam) which uses the DQN loss gradients
+    #in a particular way to compute backpropagation.
+    buffer_size = config["buffer_size"] #defining the max size of the replay buffer.
+    replay_buffer = ReplayBuffer(buffer_size) #instantiating the replay buffer.
+    batch_size = config["batch_size"] #defining the batch size
+    num_frames = config["num_total_frames"] #defining the run length (total frames over all episodes).
+    #training the agent DQN as a function of: env[domain (agent+environment physics)+task on the domain],
+    #DQN architecture, replay buffer size, batch size, optimizer, total length of run in frames.
+    train_CartPole_DQNet(env, net, replay_buffer, batch_size, optimizer, num_frames)
+    ###########################
+    run.finish() #ending the WandB project run. Logging completed.
+    ###########################
\ No newline at end of file
