diff --git a/Modern Implementations/DQN_etc/DQN_Classes.py b/Modern Implementations/DQN_etc/DQN_Classes.py
index 91fa580..3b2a065 100644
--- a/Modern Implementations/DQN_etc/DQN_Classes.py	
+++ b/Modern Implementations/DQN_etc/DQN_Classes.py	
@@ -6,10 +6,13 @@ from SDP_env_Class import SDP_env
 import torch
 import torch.nn as nn
 import torch.optim as optim
-from torch.autograd import Variable
+from torch.autograd import Variable #deprecated:
+#The Variable API has been deprecated: Variables are no longer necessary to use autograd with tensors.
 import matplotlib.pyplot as plt
 import gym
 import wandb
+from stable_baselines3.common.monitor import Monitor
+from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder
 
 
 class ReplayBuffer(object):
@@ -56,7 +59,7 @@ class DQNet(nn.Module):
 
 
 def to_onehot(x):
-    oh = np.zeros(6)
+    oh = np.zeros(4)
     oh[x - 1] = 1.
     return oh
 
@@ -89,12 +92,13 @@ def DQNet_update(model, optimizer, replay_buffer, batch_size):
 
     loss = (q_value - Variable(expected_q_value.data)).pow(2).mean()
 
-    optimizer.zero_grad()
-    loss.backward()
-    optimizer.step()
+    optimizer.zero_grad() #"Sets the gradients of all optimized 'torch.Tensor' s to zero."
+    loss.backward() #"Computes the sum of gradients of given tensors with respect to [model] graph leaves."
+    optimizer.step() #"Performs a single optimization step (parameter update)."
     return loss
 
-def train_DQN(env,model,replay_buffer,batch_size,optimizer, num_frames, n_avg):
+def train_DQN(env, model, replay_buffer, batch_size, optimizer, num_frames, n_avg):
+    wandb.watch(model, log="all", log_freq=10)
     losses = []
     all_rewards = []
     episode_reward = 0
@@ -111,6 +115,8 @@ def train_DQN(env,model,replay_buffer,batch_size,optimizer, num_frames, n_avg):
         episode_reward += reward
 
         if done:
+            metric_name = "reward"
+            train_log(metric_name, episode_reward, frame_idx)
             state = env.reset()
             all_rewards.append(episode_reward)
             episode_reward = 0
@@ -118,12 +124,20 @@ def train_DQN(env,model,replay_buffer,batch_size,optimizer, num_frames, n_avg):
         if len(replay_buffer) > batch_size:
             loss = DQNet_update(model, optimizer, replay_buffer, batch_size)
             losses.append(loss.data)
+            if frame_idx % (num_frames/100) == 0:
+                metric_name = "loss"
+                train_log(metric_name,loss,frame_idx)
 
         #if frame_idx % num_frames == 0:
             #plot(frame_idx, all_rewards, losses)
 
     train_DQN_plot(all_rewards, n_avg, num_frames)
 
+def train_log(metric_name, metric_val, frame_idx):
+    metric_val = float(metric_val)
+    wandb.log({metric_name: metric_val}, step=frame_idx)
+    print("Logging "+str(metric_name)+" of: "+str(metric_val)+", at frame index: "+str(frame_idx)+", to WandB")
+
 def train_DQN_plot(all_rewards, n, num_frames):
     plt.figure(figsize=(20, 5))
     plt.title("DQN: mean rewards over next "+str(n)+" episodes (" + str(num_frames) + " total frames)")
@@ -136,6 +150,7 @@ def train_DQN_plot(all_rewards, n, num_frames):
 
 def train_h_DQN(env, meta_model, model, meta_replay_buffer, replay_buffer,
                 batch_size,meta_optimizer, optimizer, num_frames, n_avg):
+    wandb.watch(model, log="all", log_freq=10)
     state = env.reset()
     frame_idx = 1
     done = False
@@ -172,11 +187,16 @@ def train_h_DQN(env, meta_model, model, meta_replay_buffer, replay_buffer,
                                            meta_replay_buffer, batch_size)
             losses.append(model_loss)
             meta_losses.append(meta_model_loss)
+            if frame_idx % (num_frames/100) == 0:
+                metric_name = "loss"
+                train_log(metric_name,loss,frame_idx)
             frame_idx += 1
 
         meta_replay_buffer.push(meta_state, goal, extrinsic_reward, state, done)
 
         if done:
+            metric_name = "reward"
+            train_log(metric_name, episode_reward, frame_idx)
             state = env.reset()
             done = False
             all_rewards.append(episode_reward)
@@ -204,11 +224,10 @@ def plot(frame_idx, rewards, losses):
     plt.plot(losses)
     plt.show()
 
-
 if __name__ == "__main__":
     #possible_outputs = ["DQN_only","h-DQN_only","DQN_h-DQN_comparison"]
-    DQN = True
-    h_DQN = False
+    DQN = False
+    h_DQN = True
     num_frames = 5000
     batch_size = 32
     buffer_size = int(num_frames/10)
@@ -221,7 +240,7 @@ if __name__ == "__main__":
     }
 
     run = wandb.init(
-        project="PyTorch", #name of project on WandB
+        project="PyTorch",  #name of project on WandB
         config=config,
         sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics
         monitor_gym=True,  # auto-upload the videos of agents playing the game
@@ -230,9 +249,9 @@ if __name__ == "__main__":
 
     #DQN
     if DQN:
-        env = gym.make(config["env_name"]) #SDP_env()
-        num_states = env.observation_space.shape[0] #env.num_states
-        num_actions = env.action_space.n #env.num_actions
+        env = gym.make(config["env_name"])
+        num_states = env.observation_space.shape[0]  #env.num_states
+        num_actions = env.action_space.n  #env.num_actions
         model = DQNet(num_states, num_actions)
         optimizer = optim.Adam(model.parameters())
         replay_buffer = ReplayBuffer(buffer_size)
@@ -241,9 +260,9 @@ if __name__ == "__main__":
     #h-DQN
     if h_DQN:
         goal_state_rep_f = 2
-        env = SDP_env()
-        num_goals = env.num_states
-        num_actions = env.num_actions
+        env = gym.make(config["env_name"]) #SDP_env()
+        num_goals = env.observation_space.shape[0]  # env.num_states
+        num_actions = env.action_space.n  # env.num_actions
         model = DQNet(goal_state_rep_f*num_goals, num_actions)
         meta_model = DQNet(num_goals, num_goals)
         optimizer = optim.Adam(model.parameters())
@@ -252,7 +271,7 @@ if __name__ == "__main__":
         meta_replay_buffer = ReplayBuffer(buffer_size)
         train_h_DQN(env,meta_model,model,meta_replay_buffer,replay_buffer,
                     batch_size,meta_optimizer,optimizer,num_frames,n_avg)
-
+    run.finish()
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 #h-DQN control sequence
 
