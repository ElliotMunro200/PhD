step: 0
-0.09112294253991508
False
0.006036180180338843
False
0.1031928680161411
False
0.200357897613985
False
0.10276895930452032
False
0.005186860879506414
False
-0.09239744851846023
False
0.00477710638149946
False
0.10194708028540204
False
0.19912321586824205
False
0.10154236576726916
False
0.19874051502649495
False
0.2959546281384737
False
0.1984166331958586
False
0.10089237080064897
False
0.0033725856949861632
False
-0.09415208830906399
False
0.003085478867342728
False
-0.09446045835745646
False
-0.19202071023366232
False
-0.09483346246404584
False
0.0023343750208944525
False
-0.09527522575214109
False
-0.1928995298288849
False
-0.290546799888985
False
-0.38822453483932795
False
-0.485939167517882
False
-0.5836957425581086
False
-0.6814975696172281
False
-0.7793458476445071
False
-0.8772392565352284
True
Logging reward of: 24.48309380081247, at frame index: 31, to WandB
-0.08143281948434394
False
-0.17866841109711828
False
-0.27592495590953636
False
-0.37321231411904743
False
-0.47053966520359175
False
-0.5679152607062826
False
-0.665346156541922
False
-0.7628379200301634
False
-0.8603943071177179
False
-0.9580169056502126
True
Logging loss of: -0.0071425228379666805, at frame index: 40, to WandB
Logging reward of: 7.711101485554681, at frame index: 41, to WandB
-0.10230275571639375
False
-0.19991950064124805
False
-0.29755357663695015
False
-0.39521301479389925
False
-0.4929049869907581
False
-0.5906354972285511
False
-0.49368967680642656
False
-0.39680878437568606
False
-0.2999833552879268
False
-0.20320310501923977
False
-0.10645712143037418
False
-0.009734039747924556
False
-0.1076924596604523
False
-0.01100047457740734
False
-0.10898163817835944
False
-0.01232539202108135
False
-0.11033201378709094
False
-0.013716410387675473
False
-0.1117511754675681
False
-0.01518141264580955
False
Logging loss of: -0.0037043574266135693, at frame index: 60, to WandB
-0.11324695347921879
False
-0.016728555728334635
False
-0.11482742049303873
False
-0.018366271366454343
False
-0.1165008823476308
False
-0.02010325580197625
False
-0.1182758553714888
False
-0.02194844500571795
False
-0.12016102641416225
False
-0.023910971137256362
False
0.07229399474492362
False
-0.02598271090173518
False
0.07015614689952704
False
-0.028154176331987857
False
0.0679104063212448
False
-0.030435394525426387
False
0.06554633241340728
False
0.16147708196425475
False
0.06307038437246663
False
0.15892736179608685
False
Logging loss of: -0.001138988882303238, at frame index: 80, to WandB
0.2547383312290527
False
0.15628066371669155
False
0.2520261588747017
False
0.34773031837399215
False
0.2492299542891446
False
0.34487616251723185
False
0.44048537736273163
False
0.3419492063695626
False
0.4375072559787349
False
0.33895439489693924
False
0.43445001461430804
False
0.3358787651813235
False
0.4313004836867931
False
0.5266724314974072
False
0.42806163132796615
False
0.5233644996951621
False
0.4247384072563042
False
0.5199599638889831
False
0.6151246696866511
False
0.5164616985881112
False
Logging loss of: 0.0004855986626353115, at frame index: 100, to WandB
0.6115483534515685
False
0.512874821452357
False
0.6078705501948404
False
0.7028001190130712
False
0.6040946968367221
False
0.6989350460568428
False
0.6002268885997388
False
0.6949647512687237
False
0.5962591105191715
False
0.690881550249998
False
0.7854143860222239
False
0.6866929903828616
False
0.7811106302979993
False
0.8754358874566268
True
Logging reward of: 57.739357018487354, at frame index: 115, to WandB
-0.09481784243254394
False
-0.19206389272325708
False
-0.2893288503955186
False
-0.19184099251410974
False
-0.2891526205100283
False
-0.19170890584697695
False
Logging loss of: -0.006470845080912113, at frame index: 120, to WandB
-0.28906617295735665
False
-0.19166688460077708
False
-0.09428553927998533
False
-0.1916957816859412
False
-0.09433972819602696
False
-0.19177494660880276
False
-0.09444457282339945
False
-0.19190485995834156
False
-0.09460071419994442
False
-0.192086305052525
False
-0.09480910251212182
False
-0.19232037154213305
False
-0.09507100171540063
False
-0.19260846034190632
False
-0.09538799557774244
False
-0.19295228982590093
False
-0.0957619950814506
False
-0.19335390319732249
False
-0.09619524709169194
False
-0.19381567690917
False
Logging loss of: -0.004018762148916721, at frame index: 140, to WandB
-0.09669034416481682
False
-0.19434032996837514
False
-0.09725023532363715
False
-0.19493093389989363
False
-0.0978782375669222
False
-0.19559092307485523
False
-0.09857804780263033
False
-0.19632410501404127
False
Traceback (most recent call last):
  File "C:\Users\Jack-Server\Documents\GitHub\PhD\Modern Implementations\MuJoCo\Testing\S_L_CartPole_DDPG_main.py", line 225, in <module>
    train(args.train_iter, agent, env, args.env, evaluate, args.validate_steps, args.output,
  File "C:\Users\Jack-Server\Documents\GitHub\PhD\Modern Implementations\MuJoCo\Testing\S_L_CartPole_DDPG_main.py", line 69, in train
    policy_loss = agent.update_policy() # calculating the loss on a batch of data
  File "C:\Users\Jack-Server\Documents\GitHub\PhD\Modern Implementations\MuJoCo\Testing\C_L_ddpg.py", line 71, in update_policy
    value_loss.backward()
  File "C:\Users\Jack-Server\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\_tensor.py", line 255, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "C:\Users\Jack-Server\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\autograd\__init__.py", line 147, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt